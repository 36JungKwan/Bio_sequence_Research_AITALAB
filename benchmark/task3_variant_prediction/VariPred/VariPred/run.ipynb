{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f112359",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class Args:\n",
    "    df_path = \"benchmark\\task3_variant_prediction\\VariPred\\example\\dataset\"\n",
    "    pred = \"target\"\n",
    "    output = \"VariPred_output\"\n",
    "    train = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cea06",
   "metadata": {},
   "source": [
    "# INFERENCE VARIPRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_csv(f\"{args.df_path}/{args.pred}.csv\")\n",
    "main.run_VariPred(target_ds=args.pred, output=args.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46384c",
   "metadata": {},
   "source": [
    "# EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08663df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculate_metrics import evaluate_performance\n",
    "res_file = 'benchmark\\task3_variant_prediction\\VariPred\\example\\output_results\\VariPred_output.txt' \n",
    "ori_file = 'benchmark\\task3_variant_prediction\\VariPred\\example\\dataset\\test.parquet'\n",
    "    \n",
    "metrics = evaluate_performance(res_file, ori_file)\n",
    "print(json.dumps(metrics, indent=4))\n",
    "\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(metrics,indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
