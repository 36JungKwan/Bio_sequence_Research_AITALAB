{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40d41a7",
   "metadata": {},
   "source": [
    "### Config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_4660\\1246572024.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  TRAIN_PARQUET = os.path.join(DATA_DIR, \"train\\task3_variant_prediction\\data\\train.parquet\")\n",
      "C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_4660\\1246572024.py:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  VAL_PARQUET = os.path.join(DATA_DIR, \"train\\task3_variant_prediction\\data\\val.parquet\")\n",
      "C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_4660\\1246572024.py:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  TEST_PARQUET = os.path.join(DATA_DIR, \"train\\task3_variant_prediction\\data\\test.parquet\")\n",
      "C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_4660\\1246572024.py:10: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  EMB_DIR = \"benchmark\\task3_variant_prediction\\HyenaDNA\\embeddings_hyenadna\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = \"data\" \n",
    "TRAIN_PARQUET = os.path.join(DATA_DIR, \"train/task3_variant_prediction/data/train.parquet\")\n",
    "VAL_PARQUET = os.path.join(DATA_DIR, \"train/task3_variant_prediction/data/val.parquet\")\n",
    "TEST_PARQUET = os.path.join(DATA_DIR, \"train/task3_variant_prediction/data/test.parquet\")\n",
    "\n",
    "# Embeddings directory\n",
    "EMB_DIR = \"benchmark/task3_variant_prediction/HyenaDNA/embeddings_hyenadna\"\n",
    "TRAIN_EMB = os.path.join(EMB_DIR, \"train_embeddings.pt\")\n",
    "VAL_EMB = os.path.join(EMB_DIR, \"val_embeddings.pt\")\n",
    "TEST_EMB = os.path.join(EMB_DIR, \"test_embeddings.pt\")\n",
    "\n",
    "# HyenaDNA model\n",
    "HYENADNA_MODEL = \"LongSafari/hyenadna-large-1m-seqlen-hf\"  # Hoặc medium-450k-seqlen\n",
    "DNA_SEQ_LEN = 601  # Max length cho DNA sequences\n",
    "\n",
    "# Embedding batch sizes\n",
    "DNA_BATCH = 128\n",
    "\n",
    "# Model hyperparameters\n",
    "PROJ_DIM = 512\n",
    "FUSION_HIDDEN = [512, 256]\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 512\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6855f31",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HyenaDNADataset(Dataset):\n",
    "    \"\"\"Dataset cho HyenaDNA embeddings\"\"\"\n",
    "    def __init__(self, pt_file):\n",
    "        data = torch.load(pt_file)\n",
    "        self.dna_ref = data[\"dna_ref\"]\n",
    "        self.dna_alt = data[\"dna_alt\"]\n",
    "        self.labels = data[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.dna_ref[idx],\n",
    "            self.dna_alt[idx],\n",
    "            self.labels[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f73e43",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f176cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModalityProjector(nn.Module):\n",
    "    \"\"\"Projector cho DNA embeddings: [ref, alt, diff] -> proj_dim\"\"\"\n",
    "    def __init__(self, emb_dim, proj_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, ref, alt):\n",
    "        diff = alt - ref\n",
    "        x = torch.cat([ref, alt, diff], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DNAClassifier(nn.Module):\n",
    "    \"\"\"MLP Classifier chỉ dùng DNA embeddings từ HyenaDNA\"\"\"\n",
    "    def __init__(self, dna_dim, proj_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.dna_proj = ModalityProjector(dna_dim, proj_dim, dropout)\n",
    "        \n",
    "        # Classifier layers\n",
    "        layers = []\n",
    "        in_dim = proj_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = h\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, dna_ref, dna_alt):\n",
    "        dna_z = self.dna_proj(dna_ref, dna_alt)\n",
    "        return self.classifier(dna_z).squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf6e64",
   "metadata": {},
   "source": [
    "### Precompute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6e5de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading HyenaDNA model: LongSafari/hyenadna-large-1m-seqlen-hf\n",
      "Warning: data\\train\task3_variant_prediction\\data\train.parquet not found, skipping...\n",
      "Warning: data\\train\task3_variant_prediction\\data\u000bal.parquet not found, skipping...\n",
      "Warning: data\\train\task3_variant_prediction\\data\test.parquet not found, skipping...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def embed_dna_hyenadna(seqs, tokenizer, model, batch_size, max_length=601):\n",
    "    \"\"\"Embed DNA sequences using HyenaDNA - lấy center token\"\"\"\n",
    "    all_embs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(seqs), batch_size), desc=\"Embedding DNA\"):\n",
    "            batch = seqs[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, output_hidden_states=True)\n",
    "            # Lấy hidden state lớp cuối cùng, token ở giữa\n",
    "            last_hidden = outputs.hidden_states[-1]  # [B, L, H]\n",
    "            seq_len = last_hidden.size(1)\n",
    "            center_idx = seq_len // 2\n",
    "            batch_embs = last_hidden[:, center_idx, :].float().cpu()\n",
    "            all_embs.append(batch_embs)\n",
    "    \n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "\n",
    "def process_split(parquet_path, out_path, tokenizer, model, batch_size):\n",
    "    \"\"\"Process một split (train/val/test)\"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"\\nProcessing {parquet_path} ({len(df)} rows)\")\n",
    "    \n",
    "    # Kiểm tra columns\n",
    "    if \"ref_seq\" not in df.columns or \"alt_seq\" not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns in {parquet_path}\")\n",
    "    \n",
    "    dna_ref = df[\"ref_seq\"].astype(str).tolist()\n",
    "    dna_alt = df[\"alt_seq\"].astype(str).tolist()\n",
    "    \n",
    "    # Kiểm tra label column\n",
    "    if \"label\" in df.columns:\n",
    "        labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "    elif \"ClinicalSignificance\" in df.columns:\n",
    "        keep = [\"Pathogenic\", \"Benign\"]\n",
    "        df = df[df[\"ClinicalSignificance\"].isin(keep)].copy()\n",
    "        label_map = {\"Pathogenic\": 1, \"Benign\": 0}\n",
    "        df[\"label\"] = df[\"ClinicalSignificance\"].map(label_map)\n",
    "        labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"No label column found in {parquet_path}\")\n",
    "    \n",
    "    # Embed DNA sequences\n",
    "    print(\"Embedding ref sequences...\")\n",
    "    dna_ref_emb = embed_dna_hyenadna(dna_ref, tokenizer, model, batch_size, DNA_SEQ_LEN)\n",
    "    \n",
    "    print(\"Embedding alt sequences...\")\n",
    "    dna_alt_emb = embed_dna_hyenadna(dna_alt, tokenizer, model, batch_size, DNA_SEQ_LEN)\n",
    "    \n",
    "    # Lưu embeddings\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"dna_ref\": dna_ref_emb,\n",
    "            \"dna_alt\": dna_alt_emb,\n",
    "            \"label\": labels,\n",
    "        },\n",
    "        out_path,\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved to {out_path}\")\n",
    "    print(f\"  DNA embedding dim: {dna_ref_emb.shape[1]}\")\n",
    "    print(f\"  Number of samples: {len(labels)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--split\", type=str, choices=[\"train\", \"val\", \"test\", \"all\"], default=\"all\",\n",
    "                       help=\"Which split to process\")\n",
    "    parser.add_argument(\"--dna_batch\", type=int, default=DNA_BATCH,\n",
    "                       help=\"Batch size for DNA embedding\")\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Loading HyenaDNA model: {HYENADNA_MODEL}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(HYENADNA_MODEL, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        HYENADNA_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    splits_to_process = []\n",
    "    if args.split == \"all\":\n",
    "        splits_to_process = [\n",
    "            (TRAIN_PARQUET, TRAIN_EMB),\n",
    "            (VAL_PARQUET, VAL_EMB),\n",
    "            (TEST_PARQUET, TEST_EMB),\n",
    "        ]\n",
    "    elif args.split == \"train\":\n",
    "        splits_to_process = [(TRAIN_PARQUET, TRAIN_EMB)]\n",
    "    elif args.split == \"val\":\n",
    "        splits_to_process = [(VAL_PARQUET, VAL_EMB)]\n",
    "    elif args.split == \"test\":\n",
    "        splits_to_process = [(TEST_PARQUET, TEST_EMB)]\n",
    "    \n",
    "    for parquet_path, emb_path in splits_to_process:\n",
    "        if not os.path.exists(parquet_path):\n",
    "            print(f\"Warning: {parquet_path} not found, skipping...\")\n",
    "            continue\n",
    "        process_split(parquet_path, emb_path, tokenizer, model, args.dna_batch)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc407d20",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4a9e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING CONFIGURATION:\n",
      "======================================================================\n",
      "  Experiment Name: experiment_1\n",
      "  Device: cuda\n",
      "  Learning Rate: 0.0001\n",
      "  Epochs: 30\n",
      "  Batch Size: 512\n",
      "  Patience: 5\n",
      "  Dropout: 0.3\n",
      "  Seed: 42\n",
      "  Proj Dim: 512\n",
      "  Fusion Hidden: [512, 256]\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'benchmark\\task3_variant_prediction\\\\HyenaDNA\\\\embeddings_hyenadna\\\\train_embeddings.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 287\u001b[39m\n\u001b[32m    284\u001b[39m parser.add_argument(\u001b[33m\"\u001b[39m\u001b[33m--log_dir\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mstr\u001b[39m, default=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    285\u001b[39m args = parser.parse_args([])\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    137\u001b[39m writer = SummaryWriter(log_dir=args.log_dir)\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m train_ds = \u001b[43mHyenaDNADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_EMB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m val_ds = HyenaDNADataset(VAL_EMB)\n\u001b[32m    142\u001b[39m test_ds = HyenaDNADataset(TEST_EMB)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mHyenaDNADataset.__init__\u001b[39m\u001b[34m(self, pt_file)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pt_file):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.dna_ref = data[\u001b[33m\"\u001b[39m\u001b[33mdna_ref\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.dna_alt = data[\u001b[33m\"\u001b[39m\u001b[33mdna_alt\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\torch\\serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'benchmark\\task3_variant_prediction\\\\HyenaDNA\\\\embeddings_hyenadna\\\\train_embeddings.pt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAUROC, BinaryAccuracy, BinaryMatthewsCorrCoef, MulticlassF1Score, MulticlassAccuracy,\n",
    "    BinaryConfusionMatrix, BinaryPrecision, BinaryRecall, BinarySpecificity\n",
    ")\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm_tensor, epoch, stage):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = cm_tensor.cpu().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Benign', 'Pathogenic'], \n",
    "                yticklabels=['Benign', 'Pathogenic'])\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(f'Confusion Matrix - {stage} - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, criterion, device, metrics_collection, cm_metric, optimizer=None, writer=None, epoch=0, stage=\"train\"):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    metrics_collection.reset()\n",
    "    cm_metric.reset()\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"{stage.capitalize()} Epoch {epoch}\", leave=False)\n",
    "\n",
    "    for dna_ref, dna_alt, label in pbar:\n",
    "        dna_ref = dna_ref.to(device)\n",
    "        dna_alt = dna_alt.to(device)\n",
    "        label = label.to(device).float()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(dna_ref, dna_alt)\n",
    "            loss = criterion(logits, label)\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(label)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        metrics_collection.update(preds, label.int())\n",
    "        cm_metric.update(preds, label.int())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    results = {k: v.item() for k, v in metrics_collection.compute().items()}\n",
    "    results['loss'] = avg_loss\n",
    "\n",
    "    if writer:\n",
    "        for name, value in results.items():\n",
    "            writer.add_scalar(f\"{stage}/{name}\", value, epoch)\n",
    "        \n",
    "        cm_tensor = cm_metric.compute()\n",
    "        fig = plot_confusion_matrix(cm_tensor, epoch, stage)\n",
    "        writer.add_figure(f\"ConfusionMatrix/{stage}\", fig, epoch)\n",
    "        plt.close(fig)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    seed_everything(args.seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Tạo experiment directory\n",
    "    if args.exp_name is None:\n",
    "        args.exp_name = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    if args.log_dir is None:\n",
    "        args.log_dir = os.path.join(\"runs_hyenadna\", args.exp_name)\n",
    "    \n",
    "    exp_dir = os.path.join(\"experiments_hyenadna\", args.exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # In configuration\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING CONFIGURATION:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Experiment Name: {args.exp_name}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Learning Rate: {args.lr}\")\n",
    "    print(f\"  Epochs: {args.epochs}\")\n",
    "    print(f\"  Batch Size: {args.batch_size}\")\n",
    "    print(f\"  Patience: {args.patience}\")\n",
    "    print(f\"  Dropout: {args.dropout}\")\n",
    "    print(f\"  Seed: {args.seed}\")\n",
    "    print(f\"  Proj Dim: {args.proj_dim}\")\n",
    "    print(f\"  Fusion Hidden: {args.fusion_hidden}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Lưu config\n",
    "    config_snapshot = {\n",
    "        \"exp_name\": args.exp_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"lr\": args.lr,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"patience\": args.patience,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"seed\": args.seed,\n",
    "        \"proj_dim\": args.proj_dim,\n",
    "        \"fusion_hidden\": args.fusion_hidden,\n",
    "    }\n",
    "    with open(os.path.join(exp_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config_snapshot, f, indent=2)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "    # Load datasets\n",
    "    train_ds = HyenaDNADataset(TRAIN_EMB)\n",
    "    val_ds = HyenaDNADataset(VAL_EMB)\n",
    "    test_ds = HyenaDNADataset(TEST_EMB)\n",
    "\n",
    "    loader_args = {'batch_size': args.batch_size, 'num_workers': 8, 'pin_memory': True}\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, **loader_args)\n",
    "    test_loader = DataLoader(test_ds, shuffle=False, **loader_args)\n",
    "\n",
    "    # Tạo model\n",
    "    dna_dim = train_ds.dna_ref.shape[1]\n",
    "    model = DNAClassifier(\n",
    "        dna_dim=dna_dim,\n",
    "        proj_dim=args.proj_dim,\n",
    "        hidden_dims=args.fusion_hidden,\n",
    "        dropout=args.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \" MODEL SUMMARY \" + \"=\"*30)\n",
    "    input_data_shapes = [\n",
    "        (args.batch_size, dna_dim),  # dna_ref\n",
    "        (args.batch_size, dna_dim),  # dna_alt\n",
    "    ]\n",
    "\n",
    "    model_stats = summary(\n",
    "        model, \n",
    "        input_size=input_data_shapes,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
    "        device=device,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(model_stats)\n",
    "    \n",
    "    summary_path = os.path.join(exp_dir, \"model_summary.txt\")\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(model_stats))\n",
    "    print(f\"--> Model summary saved to {summary_path}\")\n",
    "    print(\"=\"*75 + \"\\n\")\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    metrics = torchmetrics.MetricCollection({\n",
    "        'auc': BinaryAUROC(),\n",
    "        'acc': BinaryAccuracy(),\n",
    "        'mcc': BinaryMatthewsCorrCoef(),\n",
    "        'balanced_acc': MulticlassAccuracy(num_classes=2, average='macro'),\n",
    "        'f1_macro': MulticlassF1Score(num_classes=2, average='macro'),\n",
    "        'precision': BinaryPrecision(),\n",
    "        'recall': BinaryRecall(),\n",
    "        'specificity': BinarySpecificity()\n",
    "    }).to(device)\n",
    "\n",
    "    cm_metric = BinaryConfusionMatrix().to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    save_path = os.path.join(exp_dir, \"best_model.pt\")\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_res = run_epoch(model, train_loader, criterion, device, metrics, cm_metric, optimizer, writer, epoch, \"train\")\n",
    "        val_res = run_epoch(model, val_loader, criterion, device, metrics, cm_metric, None, writer, epoch, \"val\")\n",
    "\n",
    "        print(f\"[{epoch}] Train Loss: {train_res['loss']:.4f} | Val Loss: {val_res['loss']:.4f} | Train Acc: {train_res['acc']:.4f} | Val Acc: {val_res['acc']:.4f}\")\n",
    "\n",
    "        scheduler.step(val_res['loss'])\n",
    "\n",
    "        if val_res['loss'] < best_val_loss:\n",
    "            best_val_loss = val_res['loss']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"--> Saved best model checkpoint to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= args.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n--- Testing with Best Model ---\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    test_res = run_epoch(model, test_loader, criterion, device, metrics, cm_metric, None, writer, args.epochs, \"test\")\n",
    "    print(f\"[TEST] Loss: {test_res['loss']:.4f} | AUC: {test_res['auc']:.4f} | MCC: {test_res['mcc']:.4f} | Acc: {test_res['acc']:.4f} | Spec: {test_res['specificity']:.4f}\")\n",
    "    print(f\"[TEST] Balanced Acc: {test_res['balanced_acc']:.4f} | F1_macro: {test_res['f1_macro']:.4f} | Precision: {test_res['precision']:.4f} | Recall: {test_res['recall']:.4f}\")\n",
    "    \n",
    "    # Lưu hparams vào TensorBoard\n",
    "    hparams = {\n",
    "        \"lr\": args.lr,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"proj_dim\": args.proj_dim,\n",
    "        \"fusion_hidden\": str(args.fusion_hidden),\n",
    "        \"patience\": args.patience,\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        \"test_auc\": test_res['auc'],\n",
    "        \"test_acc\": test_res['acc'],\n",
    "        \"test_mcc\": test_res['mcc'],\n",
    "        \"test_balanced_acc\": test_res['balanced_acc'],\n",
    "        \"test_f1_macro\": test_res['f1_macro'],\n",
    "        \"test_precision\": test_res['precision'],\n",
    "        \"test_recall\": test_res['recall'],\n",
    "        \"test_specificity\": test_res['specificity'],\n",
    "        \"test_loss\": test_res['loss'],\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n",
    "    writer.add_hparams(hparams, metrics_dict)\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    final_results = {\n",
    "        \"exp_name\": args.exp_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"test_results\": {k: float(v) for k, v in test_res.items()},\n",
    "        \"epochs_trained\": epoch,\n",
    "        \"hparams\": hparams,\n",
    "    }\n",
    "    with open(os.path.join(exp_dir, \"results.json\"), \"w\") as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"\\n✓ Experiment saved to: {exp_dir}\")\n",
    "    print(f\"  - Config: {os.path.join(exp_dir, 'config.json')}\")\n",
    "    print(f\"  - Results: {os.path.join(exp_dir, 'results.json')}\")\n",
    "    print(f\"  - Model: {save_path}\")\n",
    "    \n",
    "    return test_res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"experiment_1\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=LR)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=EPOCHS)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=BATCH_SIZE)\n",
    "    parser.add_argument(\"--patience\", type=int, default=PATIENCE)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=DROPOUT)\n",
    "    parser.add_argument(\"--seed\", type=int, default=SEED)\n",
    "    parser.add_argument(\"--proj_dim\", type=int, default=PROJ_DIM)\n",
    "    parser.add_argument(\"--fusion_hidden\", type=int, nargs='+', default=FUSION_HIDDEN)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=WEIGHT_DECAY)\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=None)\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
