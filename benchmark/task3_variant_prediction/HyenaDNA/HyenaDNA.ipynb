{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40d41a7",
   "metadata": {},
   "source": [
    "### Config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = \"data\" \n",
    "TRAIN_PARQUET = os.path.join(DATA_DIR, \"train.parquet\")\n",
    "VAL_PARQUET = os.path.join(DATA_DIR, \"val.parquet\")\n",
    "TEST_PARQUET = os.path.join(DATA_DIR, \"test.parquet\")\n",
    "\n",
    "# Embeddings directory\n",
    "EMB_DIR = \"embeddings_hyenadna\"\n",
    "TRAIN_EMB = os.path.join(EMB_DIR, \"train_embeddings.pt\")\n",
    "VAL_EMB = os.path.join(EMB_DIR, \"val_embeddings.pt\")\n",
    "TEST_EMB = os.path.join(EMB_DIR, \"test_embeddings.pt\")\n",
    "\n",
    "# HyenaDNA model\n",
    "HYENADNA_MODEL = \"LongSafari/hyenadna-large-1m-seqlen-hf\"  # Hoặc medium-450k-seqlen\n",
    "DNA_SEQ_LEN = 601  # Max length cho DNA sequences\n",
    "\n",
    "# Embedding batch sizes\n",
    "DNA_BATCH = 128\n",
    "\n",
    "# Model hyperparameters\n",
    "PROJ_DIM = 512\n",
    "FUSION_HIDDEN = [512, 256]\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 512\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6855f31",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HyenaDNADataset(Dataset):\n",
    "    \"\"\"Dataset cho HyenaDNA embeddings\"\"\"\n",
    "    def __init__(self, pt_file):\n",
    "        data = torch.load(pt_file)\n",
    "        self.dna_ref = data[\"dna_ref\"]\n",
    "        self.dna_alt = data[\"dna_alt\"]\n",
    "        self.labels = data[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.dna_ref[idx],\n",
    "            self.dna_alt[idx],\n",
    "            self.labels[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f73e43",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f176cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ModalityProjector(nn.Module):\n",
    "    \"\"\"Projector cho DNA embeddings: [ref, alt, diff] -> proj_dim\"\"\"\n",
    "    def __init__(self, emb_dim, proj_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 3, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, ref, alt):\n",
    "        diff = alt - ref\n",
    "        x = torch.cat([ref, alt, diff], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DNAClassifier(nn.Module):\n",
    "    \"\"\"MLP Classifier chỉ dùng DNA embeddings từ HyenaDNA\"\"\"\n",
    "    def __init__(self, dna_dim, proj_dim, hidden_dims, dropout):\n",
    "        super().__init__()\n",
    "        self.dna_proj = ModalityProjector(dna_dim, proj_dim, dropout)\n",
    "        \n",
    "        # Classifier layers\n",
    "        layers = []\n",
    "        in_dim = proj_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = h\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, dna_ref, dna_alt):\n",
    "        dna_z = self.dna_proj(dna_ref, dna_alt)\n",
    "        return self.classifier(dna_z).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf6e64",
   "metadata": {},
   "source": [
    "### Precompute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config_hyenadna import (\n",
    "    TRAIN_PARQUET,\n",
    "    VAL_PARQUET,\n",
    "    TEST_PARQUET,\n",
    "    EMB_DIR,\n",
    "    TRAIN_EMB,\n",
    "    VAL_EMB,\n",
    "    TEST_EMB,\n",
    "    HYENADNA_MODEL,\n",
    "    DNA_BATCH,\n",
    "    DNA_SEQ_LEN,\n",
    ")\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def embed_dna_hyenadna(seqs, tokenizer, model, batch_size, max_length=601):\n",
    "    \"\"\"Embed DNA sequences using HyenaDNA - lấy center token\"\"\"\n",
    "    all_embs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(seqs), batch_size), desc=\"Embedding DNA\"):\n",
    "            batch = seqs[i : i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, output_hidden_states=True)\n",
    "            # Lấy hidden state lớp cuối cùng, token ở giữa\n",
    "            last_hidden = outputs.hidden_states[-1]  # [B, L, H]\n",
    "            seq_len = last_hidden.size(1)\n",
    "            center_idx = seq_len // 2\n",
    "            batch_embs = last_hidden[:, center_idx, :].float().cpu()\n",
    "            all_embs.append(batch_embs)\n",
    "    \n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "\n",
    "def process_split(parquet_path, out_path, tokenizer, model, batch_size):\n",
    "    \"\"\"Process một split (train/val/test)\"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"\\nProcessing {parquet_path} ({len(df)} rows)\")\n",
    "    \n",
    "    # Kiểm tra columns\n",
    "    if \"ref_seq\" not in df.columns or \"alt_seq\" not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns in {parquet_path}\")\n",
    "    \n",
    "    dna_ref = df[\"ref_seq\"].astype(str).tolist()\n",
    "    dna_alt = df[\"alt_seq\"].astype(str).tolist()\n",
    "    \n",
    "    # Kiểm tra label column\n",
    "    if \"label\" in df.columns:\n",
    "        labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "    elif \"ClinSigSimple\" in df.columns:\n",
    "        # Map -1,0,1 -> 0,1 (nếu cần)\n",
    "        label_map = {-1: 0, 0: 0, 1: 1}  # Hoặc điều chỉnh theo nhu cầu\n",
    "        labels = torch.tensor([label_map.get(x, 0) for x in df[\"ClinSigSimple\"].values], dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"No label column found in {parquet_path}\")\n",
    "    \n",
    "    # Embed DNA sequences\n",
    "    print(\"Embedding ref sequences...\")\n",
    "    dna_ref_emb = embed_dna_hyenadna(dna_ref, tokenizer, model, batch_size, DNA_SEQ_LEN)\n",
    "    \n",
    "    print(\"Embedding alt sequences...\")\n",
    "    dna_alt_emb = embed_dna_hyenadna(dna_alt, tokenizer, model, batch_size, DNA_SEQ_LEN)\n",
    "    \n",
    "    # Lưu embeddings\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"dna_ref\": dna_ref_emb,\n",
    "            \"dna_alt\": dna_alt_emb,\n",
    "            \"label\": labels,\n",
    "        },\n",
    "        out_path,\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved to {out_path}\")\n",
    "    print(f\"  DNA embedding dim: {dna_ref_emb.shape[1]}\")\n",
    "    print(f\"  Number of samples: {len(labels)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--split\", type=str, choices=[\"train\", \"val\", \"test\", \"all\"], default=\"all\",\n",
    "                       help=\"Which split to process\")\n",
    "    parser.add_argument(\"--dna_batch\", type=int, default=DNA_BATCH,\n",
    "                       help=\"Batch size for DNA embedding\")\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Loading HyenaDNA model: {HYENADNA_MODEL}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(HYENADNA_MODEL, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        HYENADNA_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    splits_to_process = []\n",
    "    if args.split == \"all\":\n",
    "        splits_to_process = [\n",
    "            (TRAIN_PARQUET, TRAIN_EMB),\n",
    "            (VAL_PARQUET, VAL_EMB),\n",
    "            (TEST_PARQUET, TEST_EMB),\n",
    "        ]\n",
    "    elif args.split == \"train\":\n",
    "        splits_to_process = [(TRAIN_PARQUET, TRAIN_EMB)]\n",
    "    elif args.split == \"val\":\n",
    "        splits_to_process = [(VAL_PARQUET, VAL_EMB)]\n",
    "    elif args.split == \"test\":\n",
    "        splits_to_process = [(TEST_PARQUET, TEST_EMB)]\n",
    "    \n",
    "    for parquet_path, emb_path in splits_to_process:\n",
    "        if not os.path.exists(parquet_path):\n",
    "            print(f\"Warning: {parquet_path} not found, skipping...\")\n",
    "            continue\n",
    "        process_split(parquet_path, emb_path, tokenizer, model, args.dna_batch)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc407d20",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAUROC, BinaryAccuracy, BinaryMatthewsCorrCoef,\n",
    "    BinaryConfusionMatrix, BinaryPrecision, BinaryRecall, BinarySpecificity\n",
    ")\n",
    "\n",
    "from dataset_hyenadna import HyenaDNADataset\n",
    "from model_hyenadna import DNAClassifier\n",
    "from config_hyenadna import (\n",
    "    TRAIN_EMB,\n",
    "    VAL_EMB,\n",
    "    TEST_EMB,\n",
    "    PROJ_DIM,\n",
    "    FUSION_HIDDEN,\n",
    "    DROPOUT,\n",
    "    LR,\n",
    "    EPOCHS,\n",
    "    PATIENCE,\n",
    "    BATCH_SIZE,\n",
    "    SEED,\n",
    "    WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm_tensor, epoch, stage):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = cm_tensor.cpu().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Benign', 'Pathogenic'], \n",
    "                yticklabels=['Benign', 'Pathogenic'])\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(f'Confusion Matrix - {stage} - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, criterion, device, metrics_collection, cm_metric, optimizer=None, writer=None, epoch=0, stage=\"train\"):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    metrics_collection.reset()\n",
    "    cm_metric.reset()\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"{stage.capitalize()} Epoch {epoch}\", leave=False)\n",
    "\n",
    "    for dna_ref, dna_alt, label in pbar:\n",
    "        dna_ref = dna_ref.to(device)\n",
    "        dna_alt = dna_alt.to(device)\n",
    "        label = label.to(device).float()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(dna_ref, dna_alt)\n",
    "            loss = criterion(logits, label)\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(label)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "        metrics_collection.update(preds, label.int())\n",
    "        cm_metric.update(preds, label.int())\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    results = {k: v.item() for k, v in metrics_collection.compute().items()}\n",
    "    results['loss'] = avg_loss\n",
    "\n",
    "    if writer:\n",
    "        for name, value in results.items():\n",
    "            writer.add_scalar(f\"{stage}/{name}\", value, epoch)\n",
    "        \n",
    "        cm_tensor = cm_metric.compute()\n",
    "        fig = plot_confusion_matrix(cm_tensor, epoch, stage)\n",
    "        writer.add_figure(f\"ConfusionMatrix/{stage}\", fig, epoch)\n",
    "        plt.close(fig)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    seed_everything(args.seed)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Tạo experiment directory\n",
    "    if args.exp_name is None:\n",
    "        args.exp_name = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    if args.log_dir is None:\n",
    "        args.log_dir = os.path.join(\"runs_hyenadna\", args.exp_name)\n",
    "    \n",
    "    exp_dir = os.path.join(\"experiments_hyenadna\", args.exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    \n",
    "    # In configuration\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TRAINING CONFIGURATION:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Experiment Name: {args.exp_name}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Learning Rate: {args.lr}\")\n",
    "    print(f\"  Epochs: {args.epochs}\")\n",
    "    print(f\"  Batch Size: {args.batch_size}\")\n",
    "    print(f\"  Patience: {args.patience}\")\n",
    "    print(f\"  Dropout: {args.dropout}\")\n",
    "    print(f\"  Seed: {args.seed}\")\n",
    "    print(f\"  Proj Dim: {args.proj_dim}\")\n",
    "    print(f\"  Fusion Hidden: {args.fusion_hidden}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Lưu config\n",
    "    config_snapshot = {\n",
    "        \"exp_name\": args.exp_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"lr\": args.lr,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"patience\": args.patience,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"seed\": args.seed,\n",
    "        \"proj_dim\": args.proj_dim,\n",
    "        \"fusion_hidden\": args.fusion_hidden,\n",
    "    }\n",
    "    with open(os.path.join(exp_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config_snapshot, f, indent=2)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=args.log_dir)\n",
    "\n",
    "    # Load datasets\n",
    "    train_ds = HyenaDNADataset(TRAIN_EMB)\n",
    "    val_ds = HyenaDNADataset(VAL_EMB)\n",
    "    test_ds = HyenaDNADataset(TEST_EMB)\n",
    "\n",
    "    loader_args = {'batch_size': args.batch_size, 'num_workers': 8, 'pin_memory': True}\n",
    "    train_loader = DataLoader(train_ds, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, **loader_args)\n",
    "    test_loader = DataLoader(test_ds, shuffle=False, **loader_args)\n",
    "\n",
    "    # Tạo model\n",
    "    dna_dim = train_ds.dna_ref.shape[1]\n",
    "    model = DNAClassifier(\n",
    "        dna_dim=dna_dim,\n",
    "        proj_dim=args.proj_dim,\n",
    "        hidden_dims=args.fusion_hidden,\n",
    "        dropout=args.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \" MODEL SUMMARY \" + \"=\"*30)\n",
    "    input_data_shapes = [\n",
    "        (args.batch_size, dna_dim),  # dna_ref\n",
    "        (args.batch_size, dna_dim),  # dna_alt\n",
    "    ]\n",
    "\n",
    "    model_stats = summary(\n",
    "        model, \n",
    "        input_size=input_data_shapes,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
    "        device=device,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(model_stats)\n",
    "    \n",
    "    summary_path = os.path.join(exp_dir, \"model_summary.txt\")\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(model_stats))\n",
    "    print(f\"--> Model summary saved to {summary_path}\")\n",
    "    print(\"=\"*75 + \"\\n\")\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    metrics = torchmetrics.MetricCollection({\n",
    "        'auc': BinaryAUROC(),\n",
    "        'acc': BinaryAccuracy(),\n",
    "        'mcc': BinaryMatthewsCorrCoef(),\n",
    "        'precision': BinaryPrecision(),\n",
    "        'recall': BinaryRecall(),\n",
    "        'specificity': BinarySpecificity()\n",
    "    }).to(device)\n",
    "\n",
    "    cm_metric = BinaryConfusionMatrix().to(device)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    save_path = os.path.join(exp_dir, \"best_model.pt\")\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_res = run_epoch(model, train_loader, criterion, device, metrics, cm_metric, optimizer, writer, epoch, \"train\")\n",
    "        val_res = run_epoch(model, val_loader, criterion, device, metrics, cm_metric, None, writer, epoch, \"val\")\n",
    "\n",
    "        print(f\"[{epoch}] Train Loss: {train_res['loss']:.4f} | Val Loss: {val_res['loss']:.4f} | Train Acc: {train_res['acc']:.4f} | Val Acc: {val_res['acc']:.4f}\")\n",
    "\n",
    "        scheduler.step(val_res['loss'])\n",
    "\n",
    "        if val_res['loss'] < best_val_loss:\n",
    "            best_val_loss = val_res['loss']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"--> Saved best model checkpoint to {save_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= args.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n--- Testing with Best Model ---\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    test_res = run_epoch(model, test_loader, criterion, device, metrics, cm_metric, None, writer, args.epochs, \"test\")\n",
    "    print(f\"[TEST] Loss: {test_res['loss']:.4f} | AUC: {test_res['auc']:.4f} | MCC: {test_res['mcc']:.4f} | Acc: {test_res['acc']:.4f}\")\n",
    "    print(f\"[TEST] Precision: {test_res['precision']:.4f} | Recall: {test_res['recall']:.4f} | Specificity: {test_res['specificity']:.4f}\")\n",
    "    \n",
    "    # Lưu hparams vào TensorBoard\n",
    "    hparams = {\n",
    "        \"lr\": args.lr,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"proj_dim\": args.proj_dim,\n",
    "        \"fusion_hidden\": str(args.fusion_hidden),\n",
    "        \"patience\": args.patience,\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        \"test_auc\": test_res['auc'],\n",
    "        \"test_acc\": test_res['acc'],\n",
    "        \"test_mcc\": test_res['mcc'],\n",
    "        \"test_precision\": test_res['precision'],\n",
    "        \"test_recall\": test_res['recall'],\n",
    "        \"test_specificity\": test_res['specificity'],\n",
    "        \"test_loss\": test_res['loss'],\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n",
    "    writer.add_hparams(hparams, metrics_dict)\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    final_results = {\n",
    "        \"exp_name\": args.exp_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"test_results\": {k: float(v) for k, v in test_res.items()},\n",
    "        \"epochs_trained\": epoch,\n",
    "        \"hparams\": hparams,\n",
    "    }\n",
    "    with open(os.path.join(exp_dir, \"results.json\"), \"w\") as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"\\n✓ Experiment saved to: {exp_dir}\")\n",
    "    print(f\"  - Config: {os.path.join(exp_dir, 'config.json')}\")\n",
    "    print(f\"  - Results: {os.path.join(exp_dir, 'results.json')}\")\n",
    "    print(f\"  - Model: {save_path}\")\n",
    "    \n",
    "    return test_res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=None)\n",
    "    parser.add_argument(\"--lr\", type=float, default=LR)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=EPOCHS)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=BATCH_SIZE)\n",
    "    parser.add_argument(\"--patience\", type=int, default=PATIENCE)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=DROPOUT)\n",
    "    parser.add_argument(\"--seed\", type=int, default=SEED)\n",
    "    parser.add_argument(\"--proj_dim\", type=int, default=PROJ_DIM)\n",
    "    parser.add_argument(\"--fusion_hidden\", type=int, nargs='+', default=FUSION_HIDDEN)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=WEIGHT_DECAY)\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
