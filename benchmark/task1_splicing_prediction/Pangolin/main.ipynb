{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b62509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5fcd9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Benchmarking: test_1_1_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [05:25<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Benchmarking: test_2_1_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:  22%|‚ñà‚ñà‚ñè       | 31/138 [01:40<05:47,  3.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m     max_d_ensemble \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(max_d_ensemble, p_d_m)\n\u001b[0;32m    115\u001b[0m     max_a_ensemble \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(max_a_ensemble, p_a_m)\n\u001b[1;32m--> 117\u001b[0m d_final \u001b[38;5;241m=\u001b[39m \u001b[43mmax_d_ensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    118\u001b[0m a_final \u001b[38;5;241m=\u001b[39m max_a_ensemble\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# FIX L·ªñI 2: X·ª≠ l√Ω x√°c su·∫•t li√™n t·ª•c v√† Ph√¢n lo·∫°i\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pyfaidx import Fasta\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from model import Pangolin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import h√†m c·ªßa b·∫°n\n",
    "from metrics import compute_metrics\n",
    "\n",
    "# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---\n",
    "DATA_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\"\n",
    "RESULTS_FOLDER = \"results\"\n",
    "MODEL_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\Pangolin\\pretrained_model\"\n",
    "FASTA_PATH = r\"D:\\Homo_sapiens.GRCh38.dna.primary_assembly.fa\"\n",
    "\n",
    "# Th√¥ng s·ªë k·ªπ thu·∫≠t\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 256\n",
    "OFFSET = -1      \n",
    "WINDOW_SIZE = 5\n",
    "DELTA_MARGIN = 0.000005  # Hi·ªáu s·ªë t·ªëi thi·ªÉu gi·ªØa D v√† A ƒë·ªÉ tin t∆∞·ªüng\n",
    "BASE_THRESHOLD = 0.9 # Ng∆∞·ª°ng c∆° b·∫£n\n",
    "\n",
    "# Ki·∫øn tr√∫c Pangolin v3\n",
    "CONTEXT_WINDOW = 15000\n",
    "HALF_WINDOW = CONTEXT_WINDOW // 2\n",
    "N_CHANNELS = 32\n",
    "W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11, 21, 21, 21, 21, 41, 41, 41, 41])\n",
    "AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4, 10, 10, 10, 10, 25, 25, 25, 25])\n",
    "\n",
    "TEST_FILES = ['test_1_1_1.csv', 'test_2_1_1.csv', 'test_4_1_1.csv', 'test_10_1_1.csv', 'test_data.csv']\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- H√ÄM H·ªñ TR·ª¢ ---\n",
    "genome = Fasta(FASTA_PATH)\n",
    "\n",
    "def get_sequence(chrom, pos):\n",
    "    chrom_key = chrom if chrom in genome.keys() else (f\"chr{chrom}\" if f\"chr{chrom}\" in genome.keys() else chrom)\n",
    "    start, end = pos - HALF_WINDOW, pos + HALF_WINDOW\n",
    "    try:\n",
    "        return genome[chrom_key][start:end].seq.upper()\n",
    "    except:\n",
    "        return \"N\" * CONTEXT_WINDOW \n",
    "\n",
    "# Gi·ªØ nguy√™n h√†m m√£ h√≥a c·ªßa b·∫°n\n",
    "IN_MAP = np.asarray([[0, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "def one_hot_encode_torch(seq, strand):\n",
    "    seq = seq.upper().replace('A', '1').replace('C', '2').replace('G', '3').replace('T', '4').replace('N', '0')\n",
    "    if strand == '-':\n",
    "        seq = np.asarray(list(map(int, list(seq[::-1]))))\n",
    "        seq = np.where(seq == 0, 0, 5 - seq)\n",
    "    else:\n",
    "        seq = np.asarray(list(map(int, list(seq))))\n",
    "    return IN_MAP[seq.astype('int8')]\n",
    "\n",
    "# --- KH·ªûI T·∫†O ENSEMBLE ---\n",
    "models = []\n",
    "WEIGHT_FILES = [f\"final.3.{i}.3.v2\" for i in range(8)]\n",
    "for weight_file in WEIGHT_FILES:\n",
    "    m = Pangolin(L=N_CHANNELS, W=W, AR=AR)\n",
    "    path = os.path.join(MODEL_FOLDER, weight_file)\n",
    "    if os.path.exists(path):\n",
    "        m.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "        m.to(DEVICE).eval()\n",
    "        models.append(m)\n",
    "\n",
    "# --- V√íNG L·∫∂P CH√çNH ---\n",
    "for file_name in TEST_FILES:\n",
    "    input_path = os.path.join(DATA_FOLDER, file_name)\n",
    "    if not os.path.exists(input_path): continue\n",
    "        \n",
    "    print(f\"üöÄ Benchmarking: {file_name}\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    y_true = df['Splicing_types'].values\n",
    "    \n",
    "    # FIX L·ªñI 1: Kh·ªüi t·∫°o danh s√°ch d·ª± ƒëo√°n\n",
    "    probs_list = []\n",
    "    preds_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Inference\"):\n",
    "        batch_df = df.iloc[i : i + BATCH_SIZE]\n",
    "        batch_seqs = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            parts = row['id'].split('_')\n",
    "            raw = get_sequence(parts[1], int(parts[2]) + OFFSET)\n",
    "            encoded = one_hot_encode_torch(raw, parts[3])\n",
    "            batch_seqs.append(torch.from_numpy(encoded).float().permute(1, 0))\n",
    "        \n",
    "        seq_tensor = torch.stack(batch_seqs).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Kh·ªüi t·∫°o tensor l∆∞u gi√° tr·ªã MAX qua 8 m√¥ h√¨nh (FIX L·ªñI 4)\n",
    "            max_d_ensemble = torch.zeros(len(batch_df)).to(DEVICE)\n",
    "            max_a_ensemble = torch.zeros(len(batch_df)).to(DEVICE)\n",
    "            \n",
    "            for m in models:\n",
    "                output = m(seq_tensor) \n",
    "                c_idx, n_t = output.shape[1] // 2, output.shape[-1] // 2\n",
    "                \n",
    "                # Sliding Window Max-pooling trong kh√¥ng gian (Window=5)\n",
    "                half_w = WINDOW_SIZE // 2\n",
    "                window_out = output[:, c_idx - half_w : c_idx + half_w + 1, :]\n",
    "                \n",
    "                # L·∫•y Max spatial v√† Max tissue cho t·ª´ng m√¥ h√¨nh\n",
    "                p_d_m = torch.max(torch.max(window_out[:, :, :n_t], dim=1)[0], dim=1)[0]\n",
    "                p_a_m = torch.max(torch.max(window_out[:, :, n_t:], dim=1)[0], dim=1)[0]\n",
    "                \n",
    "                # FIX L·ªñI 4: L·∫•y Max qua c√°c m√¥ h√¨nh thay v√¨ Mean\n",
    "                max_d_ensemble = torch.max(max_d_ensemble, p_d_m)\n",
    "                max_a_ensemble = torch.max(max_a_ensemble, p_a_m)\n",
    "            \n",
    "            d_final = max_d_ensemble.cpu().numpy()\n",
    "            a_final = max_a_ensemble.cpu().numpy()\n",
    "            \n",
    "            # FIX L·ªñI 2: X·ª≠ l√Ω x√°c su·∫•t li√™n t·ª•c v√† Ph√¢n lo·∫°i\n",
    "            for d, a in zip(d_final, a_final):\n",
    "                diff = abs(d - a)\n",
    "                \n",
    "                # T√≠nh to√°n x√°c su·∫•t l·ªõp Neither (0) m·ªôt c√°ch t·ª± nhi√™n\n",
    "                # N·∫øu d v√† a th·∫•p, p_neither s·∫Ω cao\n",
    "                sum_da = d + a\n",
    "                p_neither = max(0.0001, 1.0 - sum_da)\n",
    "                \n",
    "                # Normalize ƒë·ªÉ t·ªïng x√°c su·∫•t b·∫±ng 1 (Duy tr√¨ AUC chu·∫©n)\n",
    "                norm_factor = p_neither + d + a\n",
    "                prob_vec = [p_neither/norm_factor, a/norm_factor, d/norm_factor] # ƒê√£ swap nh√£n 1-2\n",
    "\n",
    "                # Logic ph√¢n lo·∫°i d√πng Delta Margin v√† Threshold\n",
    "                if diff < DELTA_MARGIN or (d < BASE_THRESHOLD and a < BASE_THRESHOLD):\n",
    "                    preds_list.append(0)\n",
    "                    probs_list.append([1.0, 0.0, 0.0]) # √âp nh√£n c·ª©ng cho Prediction\n",
    "                else:\n",
    "                    if d > a:\n",
    "                        preds_list.append(2) # d (Donor model) -> Acceptor (2)\n",
    "                        probs_list.append(prob_vec)\n",
    "                    else:\n",
    "                        preds_list.append(1) # a (Acceptor model) -> Donor (1)\n",
    "                        probs_list.append(prob_vec)\n",
    "\n",
    "    probs = np.array(probs_list)\n",
    "    y_pred = np.array(preds_list)\n",
    "    \n",
    "    res = compute_metrics(y_true, y_pred, probs)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
    "    \n",
    "    final = {\"metrics\": res, \"confusion_matrix\": cm.tolist()}\n",
    "    output_path = os.path.join(RESULTS_FOLDER, file_name.replace('.csv', '_results.json'))\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(final, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ Benchmark ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68deb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID (Pos)             | Type   | Motif (Center)  | Avg_D    | Avg_A    | Pred\n",
      "-------------------------------------------------------------------------------------\n",
      "63733353             | 1      |      GGTG       | 0.9945 | 0.9947 | 2\n",
      "46150118             | 1      |      ACCA       | 0.9946 | 0.9942 | 1\n",
      "26842338             | 1      |      ACCT       | 0.9946 | 0.9949 | 2\n",
      "47786047             | 1      |      ACTC       | 0.9942 | 0.9947 | 2\n",
      "34017506             | 1      |      GGTA       | 0.9942 | 0.9936 | 1\n",
      "32743755             | 2      |      CCTA       | 0.9945 | 0.9945 | 2\n",
      "46137074             | 2      |      TCTG       | 0.9945 | 0.9941 | 1\n",
      "33819240             | 2      |      AGCA       | 0.9945 | 0.9944 | 1\n",
      "3035587              | 2      |      AGAA       | 0.9946 | 0.9948 | 2\n",
      "45455744             | 2      |      AGAG       | 0.9938 | 0.9939 | 2\n"
     ]
    }
   ],
   "source": [
    "def diagnostic_check(file_name, num_samples=5):\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "    \n",
    "    # L·∫•y m·∫´u Donor (l·ªõp 1) v√† Acceptor (l·ªõp 2)\n",
    "    donor_samples = df[df['Splicing_types'] == 1].head(num_samples)\n",
    "    acc_samples = df[df['Splicing_types'] == 2].head(num_samples)\n",
    "    diagnose_df = pd.concat([donor_samples, acc_samples])\n",
    "\n",
    "    print(f\"{'ID (Pos)':<20} | {'Type':<6} | {'Motif (Center)':<15} | {'Avg_D':<8} | {'Avg_A':<8} | {'Pred'}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    # Ng∆∞·ª°ng Threshold th·ª±c t·∫ø ƒë·ªÉ ch·∫©n ƒëo√°n\n",
    "    HARD_THRESHOLD = 0.1 \n",
    "\n",
    "    for _, row in diagnose_df.iterrows():\n",
    "        parts = row['id'].split('_')\n",
    "        raw_seq = get_sequence(parts[1], int(parts[2]))\n",
    "        \n",
    "        # 1. Ki·ªÉm tra Motif t·∫°i v·ªã tr√≠ trung t√¢m (7500)\n",
    "        # L·∫•y 4nt quanh ƒëi·ªÉm c·∫Øt ƒë·ªÉ d·ªÖ quan s√°t (V·ªã tr√≠ 7498 ƒë·∫øn 7502)\n",
    "        center_motif = raw_seq[HALF_WINDOW-2 : HALF_WINDOW+2] \n",
    "        \n",
    "        # 2. S·ª≠a l·ªói AttributeError: Chuy·ªÉn NumPy sang Tensor tr∆∞·ªõc khi unsqueeze\n",
    "        # V√¨ h√†m c·ªßa b·∫°n tr·∫£ v·ªÅ NumPy array (L, 4), ta c·∫ßn permute(1, 0) ƒë·ªÉ th√†nh (4, L)\n",
    "        encoded_seq = one_hot_encode_torch(raw_seq, parts[3])\n",
    "        seq_tensor = torch.from_numpy(encoded_seq).float().permute(1, 0).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        sum_d, sum_a = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for m in models:\n",
    "                out = m(seq_tensor) # Output shape: [1, Out_Len, 2*Tissues]\n",
    "                \n",
    "                c_idx = out.shape[1] // 2\n",
    "                n_t = out.shape[-1] // 2\n",
    "                \n",
    "                # T√°ch Donor v√† Acceptor t·ª´ tensor g·ªôp\n",
    "                p_d = out[0, c_idx, :n_t]\n",
    "                p_a = out[0, c_idx, n_t:]\n",
    "                \n",
    "                sum_d += torch.max(p_d).item()\n",
    "                sum_a += torch.max(p_a).item()\n",
    "        \n",
    "        avg_d, avg_a = sum_d/len(models), sum_a/len(models)\n",
    "        \n",
    "        # 3. √Åp d·ª•ng Threshold th·ª±c t·∫ø ƒë·ªÉ x√°c ƒë·ªãnh nh√£n d·ª± ƒëo√°n\n",
    "        if avg_d < HARD_THRESHOLD and avg_a < HARD_THRESHOLD:\n",
    "            pred = 0 # Neither\n",
    "        elif avg_d > avg_a:\n",
    "            pred = 1 # Donor\n",
    "        else:\n",
    "            pred = 2 # Acceptor\n",
    "            \n",
    "        print(f\"{parts[2]:<20} | {row['Splicing_types']:<6} | {center_motif:^15} | {avg_d:.4f} | {avg_a:.4f} | {pred}\")\n",
    "\n",
    "# G·ªçi h√†m ki·ªÉm tra\n",
    "diagnostic_check(TEST_FILES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0abca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ki·ªÉm tra Offset cho Neg t·∫°i 46105846 (+) ---\n",
      "Offset -3: AT \n",
      "Offset -2: TG \n",
      "Offset -1: GC \n",
      "Offset  0: CA \n",
      "Offset  1: AA \n",
      "Offset  2: AA \n",
      "Offset  3: AA \n"
     ]
    }
   ],
   "source": [
    "def find_best_offset(file_name, sample_idx=0):\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "    row = df.iloc[sample_idx]\n",
    "    parts = row['id'].split('_')\n",
    "    chrom, pos, strand = parts[1], int(parts[2]), parts[3]\n",
    "    \n",
    "    print(f\"--- Ki·ªÉm tra Offset cho {parts[0]} t·∫°i {pos} ({strand}) ---\")\n",
    "    # Ki·ªÉm tra trong kho·∫£ng offset t·ª´ -3 ƒë·∫øn +3\n",
    "    for offset in range(-3, 4):\n",
    "        actual_pos = pos + offset\n",
    "        raw_seq = get_sequence(chrom, actual_pos)\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t 2nt ngay t·∫°i ƒëi·ªÉm c·∫Øt (v·ªã tr√≠ 7500-7501)\n",
    "        # L∆∞u √Ω: V·ªõi strand '-', ta l·∫•y reverse complement c·ªßa v√πng ƒë√≥\n",
    "        center_2nt = raw_seq[HALF_WINDOW : HALF_WINDOW+2]\n",
    "        \n",
    "        # Donor chu·∫©n l√† GT, Acceptor chu·∫©n l√† AG\n",
    "        status = \"MATCH!\" if (row['Splicing_types'] == 1 and center_2nt == \"GT\") or \\\n",
    "                             (row['Splicing_types'] == 2 and center_2nt == \"AG\") else \"\"\n",
    "        \n",
    "        print(f\"Offset {offset:>2}: {center_2nt} {status}\")\n",
    "\n",
    "# Ch·∫°y th·ª≠ ƒë·ªÉ t√¨m offset ƒë√∫ng\n",
    "find_best_offset(TEST_FILES[0], sample_idx=0) # Th·ª≠ v·ªõi m·∫´u Donor ƒë·∫ßu ti√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85487ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ki·ªÉm tra Offset cho Donor t·∫°i 63733353 (+) ---\n",
      "Offset -3: CG \n",
      "Offset -2: GG \n",
      "Offset -1: GT MATCH!\n",
      "Offset  0: TG \n",
      "Offset  1: GA \n",
      "Offset  2: AG \n",
      "Offset  3: GG \n"
     ]
    }
   ],
   "source": [
    "def find_best_offset(file_name, sample_idx=0):\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "    row = df.iloc[sample_idx]\n",
    "    parts = row['id'].split('_')\n",
    "    chrom, pos, strand = parts[1], int(parts[2]), parts[3]\n",
    "    \n",
    "    print(f\"--- Ki·ªÉm tra Offset cho {parts[0]} t·∫°i {pos} ({strand}) ---\")\n",
    "    # Ki·ªÉm tra trong kho·∫£ng offset t·ª´ -3 ƒë·∫øn +3\n",
    "    for offset in range(-3, 4):\n",
    "        actual_pos = pos + offset\n",
    "        raw_seq = get_sequence(chrom, actual_pos)\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t 2nt ngay t·∫°i ƒëi·ªÉm c·∫Øt (v·ªã tr√≠ 7500-7501)\n",
    "        # L∆∞u √Ω: V·ªõi strand '-', ta l·∫•y reverse complement c·ªßa v√πng ƒë√≥\n",
    "        center_2nt = raw_seq[HALF_WINDOW : HALF_WINDOW+2]\n",
    "        \n",
    "        # Donor chu·∫©n l√† GT, Acceptor chu·∫©n l√† AG\n",
    "        status = \"MATCH!\" if (row['Splicing_types'] == 1 and center_2nt == \"GT\") or \\\n",
    "                             (row['Splicing_types'] == 2 and center_2nt == \"AG\") else \"\"\n",
    "        \n",
    "        print(f\"Offset {offset:>2}: {center_2nt} {status}\")\n",
    "\n",
    "# Ch·∫°y th·ª≠ ƒë·ªÉ t√¨m offset ƒë√∫ng\n",
    "find_best_offset(TEST_FILES[0], sample_idx=2) # Th·ª≠ v·ªõi m·∫´u Donor ƒë·∫ßu ti√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be21f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ki·ªÉm tra Offset cho Acc t·∫°i 32743755 (-) ---\n",
      "Offset -3: AC \n",
      "Offset -2: CC \n",
      "Offset -1: CT \n",
      "Offset  0: TA \n",
      "Offset  1: AA \n",
      "Offset  2: AA \n",
      "Offset  3: AA \n"
     ]
    }
   ],
   "source": [
    "def find_best_offset(file_name, sample_idx=0):\n",
    "    df = pd.read_csv(os.path.join(DATA_FOLDER, file_name))\n",
    "    row = df.iloc[sample_idx]\n",
    "    parts = row['id'].split('_')\n",
    "    chrom, pos, strand = parts[1], int(parts[2]), parts[3]\n",
    "    \n",
    "    print(f\"--- Ki·ªÉm tra Offset cho {parts[0]} t·∫°i {pos} ({strand}) ---\")\n",
    "    # Ki·ªÉm tra trong kho·∫£ng offset t·ª´ -3 ƒë·∫øn +3\n",
    "    for offset in range(-3, 4):\n",
    "        actual_pos = pos + offset\n",
    "        raw_seq = get_sequence(chrom, actual_pos)\n",
    "        \n",
    "        # Tr√≠ch xu·∫•t 2nt ngay t·∫°i ƒëi·ªÉm c·∫Øt (v·ªã tr√≠ 7500-7501)\n",
    "        # L∆∞u √Ω: V·ªõi strand '-', ta l·∫•y reverse complement c·ªßa v√πng ƒë√≥\n",
    "        center_2nt = raw_seq[HALF_WINDOW : HALF_WINDOW+2]\n",
    "        \n",
    "        # Donor chu·∫©n l√† GT, Acceptor chu·∫©n l√† AG\n",
    "        status = \"MATCH!\" if (row['Splicing_types'] == 1 and center_2nt == \"GT\") or \\\n",
    "                             (row['Splicing_types'] == 2 and center_2nt == \"AG\") else \"\"\n",
    "        \n",
    "        print(f\"Offset {offset:>2}: {center_2nt} {status}\")\n",
    "\n",
    "# Ch·∫°y th·ª≠ ƒë·ªÉ t√¨m offset ƒë√∫ng\n",
    "find_best_offset(TEST_FILES[0], sample_idx=3) # Th·ª≠ v·ªõi m·∫´u Donor ƒë·∫ßu ti√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e81c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
