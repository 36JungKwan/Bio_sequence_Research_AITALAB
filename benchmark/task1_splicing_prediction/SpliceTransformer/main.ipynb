{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396c9c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file 'metrics.py'. S·∫Ω d√πng h√†m metrics ƒë∆°n gi·∫£n.\n",
      "‚úÖ ƒê√£ import th√†nh c√¥ng class 'SpTransformer' t·ª´ model.py\n",
      "‚ùå Kh√¥ng t√¨m th·∫•y file weights t·∫°i: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import importlib.util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (ƒê√É C·∫¨P NH·∫¨T T·ª™ K·∫æT QU·∫¢ QU√âT)\n",
    "# ==========================================\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file ch·ª©a code model (L·∫•y t·ª´ k·∫øt qu·∫£ scan c·ªßa b·∫°n)\n",
    "MODEL_CODE_FILE = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\"\n",
    "\n",
    "# T√™n Class Model (L·∫•y t·ª´ k·∫øt qu·∫£ scan)\n",
    "MODEL_CLASS_NAME = \"SpTransformer\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file weights (.ckpt)\n",
    "CKPT_PATH = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\model.ckpt\"\n",
    "\n",
    "# Th∆∞ m·ª•c ch·ª©a 4 file csv prepared\n",
    "DATA_DIR = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\data_test\"\n",
    "\n",
    "# Th∆∞ m·ª•c xu·∫•t k·∫øt qu·∫£\n",
    "RESULT_DIR = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\results\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ratios = [\"1_1_1\", \"2_1_1\", \"4_1_1\", \"10_1_1\", \"100_1_1\"]\n",
    "\n",
    "# ==========================================\n",
    "# 2. H√ÄM LOAD MODEL T·ª™ FILE B·∫§T K·ª≤\n",
    "# ==========================================\n",
    "def load_model_class(file_path, class_name):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y file code t·∫°i: {file_path}\")\n",
    "    \n",
    "    # Load module t·ª´ ƒë∆∞·ªùng d·∫´n file\n",
    "    spec = importlib.util.spec_from_file_location(\"dynamic_model_module\", file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"dynamic_model_module\"] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    \n",
    "    if hasattr(module, class_name):\n",
    "        print(f\"‚úÖ ƒê√£ import th√†nh c√¥ng class '{class_name}' t·ª´ {os.path.basename(file_path)}\")\n",
    "        return getattr(module, class_name)\n",
    "    else:\n",
    "        raise AttributeError(f\"‚ùå Trong file '{file_path}' kh√¥ng c√≥ class '{class_name}'\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. H√ÄM LOAD CHECKPOINT (.ckpt / .pth)\n",
    "# ==========================================\n",
    "def load_weights(model, ckpt_path):\n",
    "    print(f\"üîÑ ƒêang load weights t·ª´: {ckpt_path}\")\n",
    "    \n",
    "    # Load checkpoint l√™n CPU/GPU\n",
    "    checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t state_dict\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['state_dict']\n",
    "        elif 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # X·ª≠ l√Ω t√™n keys (x√≥a prefix th·ª´a)\n",
    "    clean_state_dict = {}\n",
    "    for key, val in state_dict.items():\n",
    "        new_key = key\n",
    "        # X√≥a c√°c ti·ªÅn t·ªë ph·ªï bi·∫øn do PyTorch Lightning ho·∫∑c DataParallel sinh ra\n",
    "        for prefix in [\"model.\", \"net.\", \"module.\", \"backbone.\"]:\n",
    "            if new_key.startswith(prefix):\n",
    "                new_key = new_key[len(prefix):]\n",
    "                break\n",
    "        clean_state_dict[new_key] = val\n",
    "\n",
    "    # Load v√†o model\n",
    "    try:\n",
    "        model.load_state_dict(clean_state_dict, strict=False)\n",
    "        print(\"‚úÖ Load weights ho√†n t·∫•t!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o khi load weights: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATASET & METRICS\n",
    "# ==========================================\n",
    "# Fallback metrics n·∫øu thi·∫øu file metrics.py\n",
    "try:\n",
    "    from metrics import compute_metrics\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file 'metrics.py'. S·∫Ω d√πng h√†m metrics ƒë∆°n gi·∫£n.\")\n",
    "    def compute_metrics(labels, preds, probs=None, k=2):\n",
    "        # T√≠nh accuracy ƒë∆°n gi·∫£n ƒë·ªÉ kh√¥ng b·ªã l·ªói code\n",
    "        acc = (labels == preds).mean()\n",
    "        return {\"accuracy\": float(acc), \"note\": \"metrics.py not found\"}\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # L·∫•y sequence v√† label\n",
    "        seq_str = str(self.df.iloc[idx]['sequence']).upper()\n",
    "        label = int(self.df.iloc[idx]['Splicing_types'])\n",
    "        \n",
    "        # One-hot encoding\n",
    "        seq_enc = np.zeros((len(seq_str), 4), dtype=np.float32)\n",
    "        for i, char in enumerate(seq_str):\n",
    "            if char in self.mapping:\n",
    "                seq_enc[i, self.mapping[char]] = 1.0\n",
    "            else:\n",
    "                seq_enc[i] = 0.25\n",
    "        \n",
    "        # Transpose (L, 4) -> (4, L) cho Model\n",
    "        return torch.tensor(seq_enc).transpose(0, 1), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN RUN\n",
    "# ==========================================\n",
    "def main():\n",
    "    if not os.path.exists(RESULT_DIR):\n",
    "        os.makedirs(RESULT_DIR)\n",
    "        \n",
    "    try:\n",
    "        # 1. Load Class Model t·ª´ file model/model.py\n",
    "        ModelClass = load_model_class(MODEL_CODE_FILE, MODEL_CLASS_NAME)\n",
    "        \n",
    "        # 2. Kh·ªüi t·∫°o Model\n",
    "        # L∆∞u √Ω: N·∫øu __init__ c·ªßa model y√™u c·∫ßu tham s·ªë, c·∫ßn truy·ªÅn v√†o ƒë√¢y. \n",
    "        # Th∆∞·ªùng SpTransformer m·∫∑c ƒë·ªãnh params chu·∫©n.\n",
    "        try:\n",
    "            model = ModelClass().to(DEVICE)\n",
    "        except TypeError as e:\n",
    "            print(f\"‚ö†Ô∏è Model y√™u c·∫ßu tham s·ªë kh·ªüi t·∫°o: {e}\")\n",
    "            print(\"üëâ ƒêang th·ª≠ kh·ªüi t·∫°o v·ªõi tham s·ªë m·∫∑c ƒë·ªãnh (n·∫øu c·∫ßn)...\")\n",
    "            # V√≠ d·ª•: model = ModelClass(maxlen=...)\n",
    "            model = ModelClass().to(DEVICE)\n",
    "\n",
    "        # 3. Load Weights\n",
    "        if os.path.exists(CKPT_PATH):\n",
    "            load_weights(model, CKPT_PATH)\n",
    "        else:\n",
    "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file weights t·∫°i: {CKPT_PATH}\")\n",
    "            return\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        # 4. Ch·∫°y v√≤ng l·∫∑p Inference\n",
    "        for ratio in ratios:\n",
    "            csv_path = os.path.join(DATA_DIR, f\"prepared_inference_{ratio}.csv\")\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"‚è© B·ªè qua {ratio}: File kh√¥ng t·ªìn t·∫°i\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n>>> üèÉ ƒêang ch·∫°y: {ratio}\")\n",
    "            dataset = InferenceDataset(csv_path)\n",
    "            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            all_labels, all_preds, all_probs = [], [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in loader:\n",
    "                    inputs = inputs.to(DEVICE)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # X·ª≠ l√Ω output n·∫øu l√† tuple/list\n",
    "                    if isinstance(outputs, (tuple, list)):\n",
    "                        outputs = outputs[0]\n",
    "                    \n",
    "                    # T√≠nh x√°c su·∫•t\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    preds = torch.argmax(probs, dim=1)\n",
    "                    \n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # T√≠nh v√† l∆∞u k·∫øt qu·∫£\n",
    "            results = compute_metrics(np.array(all_labels), np.array(all_preds), np.array(all_probs))\n",
    "            \n",
    "            out_file = os.path.join(RESULT_DIR, f\"results_{ratio}.json\")\n",
    "            with open(out_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "            print(f\"   ‚úÖ Ho√†n t·∫•t. Accuracy: {results.get('accuracy', 0):.4f}\")\n",
    "            print(f\"   üìÇ K·∫øt qu·∫£ l∆∞u t·∫°i: {out_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå L·ªñI TRONG QU√Å TR√åNH CH·∫†Y: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cc9900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y INFERENCE...\n",
      "‚úÖ ƒê√£ load metrics t·ª´: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\metrics.py\n",
      "üîÑ ƒêang load weights t·ª´: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpTransformer_pytorch.ckpt\n",
      "‚ö†Ô∏è C·∫£nh b√°o load weight: Error(s) in loading state_dict for SpTransformer:\n",
      "\tsize mismatch for conv1.0.weight: copying a param with shape torch.Size([128, 4, 1]) from checkpoint, the shape in current model is torch.Size([32, 4, 1]).\n",
      "\tsize mismatch for conv1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for conv1.1.weight: copying a param with shape torch.Size([128, 128, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1]).\n",
      "\tsize mismatch for conv1.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n",
      "\tsize mismatch for conv2.weight: copying a param with shape torch.Size([256, 320, 1]) from checkpoint, the shape in current model is torch.Size([64, 224, 1]).\n",
      "\tsize mismatch for conv2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.0.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.1.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.2.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.3.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.4.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.fn.to_kv.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.fn.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.fn.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.fn.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([64, 256]).\n",
      "\tsize mismatch for attn.attn.layers.layers.5.1.fn.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for attn.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n",
      "\tsize mismatch for splice.weight: copying a param with shape torch.Size([3, 256, 1]) from checkpoint, the shape in current model is torch.Size([3, 64, 1]).\n",
      "\tsize mismatch for usage.weight: copying a param with shape torch.Size([15, 256, 1]) from checkpoint, the shape in current model is torch.Size([15, 64, 1]).\n",
      "\n",
      ">>> üèÉ ƒêang x·ª≠ l√Ω: 1_1_1 (Len: 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24052\\288485761.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå L·ªói t√≠nh metrics cho \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratio\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    177\u001b[0m all_labels, all_preds, all_probs \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    181\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    183\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\splice_transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\splice_transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\splice_transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\envs\\splice_transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 119\u001b[0m, in \u001b[0;36mInferenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    116\u001b[0m         seq_enc[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;66;03m# Gi√° tr·ªã cho N\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Transpose (L, 4) -> (4, L)\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_enc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import importlib.util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (ƒê√É GI·ªÆ NGUY√äN C·ª¶A B·∫†N)\n",
    "# ==========================================\n",
    "\n",
    "METRICS_FILE_PATH = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\metrics.py\"\n",
    "MODEL_CODE_FILE = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\"\n",
    "MODEL_CLASS_NAME = \"SpTransformer\"\n",
    "CKPT_PATH = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpTransformer_pytorch.ckpt\"\n",
    "DATA_DIR = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\data\"\n",
    "RESULT_DIR = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\results\"\n",
    "\n",
    "# --- C·∫§U H√åNH M·ªöI: ƒê·ªò D√ÄI CHU·ªñI ---\n",
    "# SpliceTransformer th∆∞·ªùng y√™u c·∫ßu 5000 ho·∫∑c 10000. \n",
    "# N·∫øu ch·∫°y v·∫´n l·ªói k√≠ch th∆∞·ªõc, h√£y th·ª≠ ƒë·ªïi s·ªë n√†y th√†nh 10000.\n",
    "MAX_SEQ_LEN = 10000  \n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ratios = [\"1_1_1\", \"2_1_1\", \"4_1_1\", \"10_1_1\", \"100_1_1\"]\n",
    "\n",
    "# ==========================================\n",
    "# 2. H√ÄM LOAD MODULE\n",
    "# ==========================================\n",
    "def load_module_from_path(file_path, module_name=\"custom_module\"):\n",
    "    \"\"\"Load m·ªôt file python nh∆∞ m·ªôt module t·ª´ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Kh√¥ng t√¨m th·∫•y file t·∫°i: {file_path}\")\n",
    "    \n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    if spec is None:\n",
    "        raise ImportError(f\"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc specs t·ª´ file: {file_path}\")\n",
    "        \n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# ==========================================\n",
    "# 3. H√ÄM LOAD WEIGHTS TH√îNG MINH\n",
    "# ==========================================\n",
    "def load_weights_smart(model, ckpt_path):\n",
    "    print(f\"üîÑ ƒêang load weights t·ª´: {ckpt_path}\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    \n",
    "    # 1. L·∫•y state_dict\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'state_dict' in checkpoint: state_dict = checkpoint['state_dict']\n",
    "        elif 'model_state_dict' in checkpoint: state_dict = checkpoint['model_state_dict']\n",
    "        else: state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # 2. X√≥a prefix th·ª´a\n",
    "    clean_state_dict = {}\n",
    "    for key, val in state_dict.items():\n",
    "        new_key = key\n",
    "        for prefix in [\"model.\", \"net.\", \"module.\", \"backbone.\"]:\n",
    "            if new_key.startswith(prefix):\n",
    "                new_key = new_key[len(prefix):]\n",
    "                break\n",
    "        clean_state_dict[new_key] = val\n",
    "\n",
    "    # 3. Load v√†o model\n",
    "    try:\n",
    "        model.load_state_dict(clean_state_dict, strict=False)\n",
    "        print(\"‚úÖ Load weights th√†nh c√¥ng!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o load weight: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATASET (ƒê√É S·ª¨A: TH√äM PADDING/CROP)\n",
    "# ==========================================\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, csv_path, max_len=MAX_SEQ_LEN):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # L·∫•y chu·ªói v√† chu·∫©n h√≥a\n",
    "        seq = str(self.df.iloc[idx]['sequence']).upper().strip()\n",
    "        label = int(self.df.iloc[idx]['Splicing_types'])\n",
    "        \n",
    "        curr_len = len(seq)\n",
    "        \n",
    "        # --- X·ª¨ L√ù ƒê·ªò D√ÄI (PADDING HO·∫∂C CROP) ---\n",
    "        if curr_len < self.max_len:\n",
    "            # N·∫øu ng·∫Øn h∆°n -> Th√™m N v√†o 2 ƒë·∫ßu (ƒë·ªÉ t√¢m ·ªü gi·ªØa)\n",
    "            pad_total = self.max_len - curr_len\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            seq = \"N\" * pad_left + seq + \"N\" * pad_right\n",
    "        elif curr_len > self.max_len:\n",
    "            # N·∫øu d√†i h∆°n -> C·∫Øt l·∫•y ƒëo·∫°n gi·ªØa\n",
    "            start = (curr_len - self.max_len) // 2\n",
    "            seq = seq[start : start + self.max_len]\n",
    "            \n",
    "        # --- ONE-HOT ENCODING ---\n",
    "        seq_enc = np.zeros((len(seq), 4), dtype=np.float32)\n",
    "        for i, base in enumerate(seq):\n",
    "            if base in self.mapping: \n",
    "                seq_enc[i, self.mapping[base]] = 1.0\n",
    "            else: \n",
    "                seq_enc[i] = 0.25 # Gi√° tr·ªã cho N\n",
    "        \n",
    "        # Transpose (L, 4) -> (4, L)\n",
    "        return torch.tensor(seq_enc).transpose(0, 1), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ==========================================\n",
    "# 5. CH∆Ø∆†NG TR√åNH CH√çNH\n",
    "# ==========================================\n",
    "def main():\n",
    "    if not os.path.exists(RESULT_DIR): os.makedirs(RESULT_DIR)\n",
    "\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y INFERENCE...\")\n",
    "\n",
    "    # A. LOAD METRICS\n",
    "    try:\n",
    "        metrics_mod = load_module_from_path(METRICS_FILE_PATH, \"metrics_mod\")\n",
    "        compute_metrics = metrics_mod.compute_metrics\n",
    "        print(f\"‚úÖ ƒê√£ load metrics t·ª´: {METRICS_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói load metrics.py: {e}\")\n",
    "        print(\"üëâ Chuy·ªÉn sang ch·∫ø ƒë·ªô t√≠nh Accuracy ƒë∆°n gi·∫£n (kh√¥ng c·∫ßn sklearn)\")\n",
    "        def compute_metrics(labels, preds, probs=None, k=2):\n",
    "            return {\"accuracy\": float((labels == preds).mean()), \"note\": \"fallback mode\"}\n",
    "\n",
    "    # B. LOAD MODEL & WEIGHTS\n",
    "    try:\n",
    "        model_mod = load_module_from_path(MODEL_CODE_FILE, \"model_mod\")\n",
    "        \n",
    "        if not hasattr(model_mod, MODEL_CLASS_NAME):\n",
    "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y class '{MODEL_CLASS_NAME}'\")\n",
    "            return\n",
    "            \n",
    "        ModelClass = getattr(model_mod, MODEL_CLASS_NAME)\n",
    "        \n",
    "        # Kh·ªüi t·∫°o model (Th·ª≠ v·ªõi tham s·ªë ƒë·ªô d√†i n·∫øu c·∫ßn)\n",
    "        try:\n",
    "            model = ModelClass().to(DEVICE)\n",
    "        except:\n",
    "            print(f\"‚ÑπÔ∏è Init m·∫∑c ƒë·ªãnh l·ªói, th·ª≠ init v·ªõi L={MAX_SEQ_LEN}...\")\n",
    "            model = ModelClass(L=MAX_SEQ_LEN).to(DEVICE)\n",
    "\n",
    "        load_weights_smart(model, CKPT_PATH)\n",
    "        model.eval()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói kh·ªüi t·∫°o Model: {e}\")\n",
    "        return\n",
    "\n",
    "    # C. CH·∫†Y V√íNG L·∫∂P\n",
    "    for ratio in ratios:\n",
    "        csv_path = os.path.join(DATA_DIR, f\"prepared_inference_{ratio}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚è© B·ªè qua {ratio}: File kh√¥ng t·ªìn t·∫°i\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n>>> üèÉ ƒêang x·ª≠ l√Ω: {ratio} (Len: {MAX_SEQ_LEN})\")\n",
    "        \n",
    "        # Truy·ªÅn max_len v√†o Dataset\n",
    "        dataset = InferenceDataset(csv_path, max_len=MAX_SEQ_LEN)\n",
    "        loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        all_labels, all_preds, all_probs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, (tuple, list)): outputs = outputs[0]\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # T√≠nh Metrics\n",
    "        try:\n",
    "            results = compute_metrics(np.array(all_labels), np.array(all_preds), np.array(all_probs))\n",
    "            \n",
    "            out_file = os.path.join(RESULT_DIR, f\"results_{ratio}.json\")\n",
    "            with open(out_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            print(f\"   ‚úÖ Xong. Acc: {results.get('accuracy', 0):.4f} | Saved: {out_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói t√≠nh metrics cho {ratio}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320d0987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang qu√©t t√¨m Model trong: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main...\n",
      "\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: ResBlock\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: SpEncoder\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: SpEncoder_4tis\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: AttnBlock\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: SpEncoder_L\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: SpEncoder2_L\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\model.py\n",
      "   üß© Class Name: SpTransformer\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\SpliceAI.py\n",
      "   üß© Class Name: ResBlock\n",
      "--------------------------------------------------\n",
      "‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\n",
      "   üìÇ File: D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\\model\\SpliceAI.py\n",
      "   üß© Class Name: SpliceAI\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c g·ªëc c·ªßa repo b·∫°n ƒë√£ t·∫£i\n",
    "SEARCH_DIR = r\"D:\\Study\\5-FA25\\AiTa_Lab_Research\\Code\\Inference_Model\\SpliceTransformer\\SpliceTransformer-main\"\n",
    "\n",
    "print(f\"üîç ƒêang qu√©t t√¨m Model trong: {SEARCH_DIR}...\\n\")\n",
    "\n",
    "found_any = False\n",
    "\n",
    "for root, dirs, files in os.walk(SEARCH_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(\".py\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(full_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    file_content = f.read()\n",
    "                    \n",
    "                    # D√πng AST ƒë·ªÉ ph√¢n t√≠ch code m√† kh√¥ng c·∫ßn import (tr√°nh l·ªói thi·∫øu th∆∞ vi·ªán)\n",
    "                    try:\n",
    "                        tree = ast.parse(file_content)\n",
    "                    except SyntaxError:\n",
    "                        continue # B·ªè qua file l·ªói c√∫ ph√°p\n",
    "\n",
    "                    for node in ast.walk(tree):\n",
    "                        if isinstance(node, ast.ClassDef):\n",
    "                            is_model = False\n",
    "                            # Ki·ªÉm tra xem class c√≥ k·∫ø th·ª´a nn.Module hay LightningModule kh√¥ng\n",
    "                            for base in node.bases:\n",
    "                                # Case 1: class Model(nn.Module) -> base l√† Attribute\n",
    "                                if isinstance(base, ast.Attribute) and base.attr in ['Module', 'LightningModule']:\n",
    "                                    is_model = True\n",
    "                                # Case 2: class Model(Module) -> base l√† Name\n",
    "                                elif isinstance(base, ast.Name) and base.id in ['Module', 'LightningModule']:\n",
    "                                    is_model = True\n",
    "                            \n",
    "                            if is_model:\n",
    "                                print(f\"‚úÖ T√åM TH·∫§Y ·ª®NG VI√äN!\")\n",
    "                                print(f\"   üìÇ File: {full_path}\")\n",
    "                                print(f\"   üß© Class Name: {node.name}\")\n",
    "                                print(\"-\" * 50)\n",
    "                                found_any = True\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "if not found_any:\n",
    "    print(\"‚ùå V·∫´n kh√¥ng t√¨m th·∫•y class n√†o k·∫ø th·ª´a Module. B·∫°n c√≥ ch·∫Øc ƒë√£ t·∫£i ƒë·ªß code kh√¥ng?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splice_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
