{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d337d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyfaidx biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a56fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch  # Chuy·ªÉn t·ª´ TF sang Torch\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from metrics import compute_metrics\n",
    "from data_preparation import prepare_csv_datasets\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi file model SPLAM (th∆∞·ªùng l√† splam_script.pt)\n",
    "MODEL_PATH = r\"D:\\my_project\\Bio_paper\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SPLAM\\pretrained_model\\splam_script.pt\"\n",
    "PREPARED_FOLDER = \"prepared_data/\"\n",
    "RESULTS_DIR = \"results/\"\n",
    "BATCH_SIZE = 256  \n",
    "CONTEXT = 400     \n",
    "TARGET_LEN = 800\n",
    "test_files = ['test_1_1_1.csv', 'test_2_1_1.csv', 'test_4_1_1.csv', 'test_10_1_1.csv', 'test_data.csv']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b7a52",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f280a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:29] Loading Genome...\n",
      "üöÄ Processing: test_1_1_1.csv (26310 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26310/26310 [00:00<00:00, 34708.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type       | Motif at Expected Index   | Status\n",
      "------------------------------------------------------------\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [TA] | ‚ùå\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "‚úÖ Saved to prepared_data/test_1_1_1.csv | Speed: 23026.48 seq/s\n",
      "\n",
      "üöÄ Processing: test_2_1_1.csv (35132 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35132/35132 [00:01<00:00, 34624.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type       | Motif at Expected Index   | Status\n",
      "------------------------------------------------------------\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [TC] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AA] | ‚ùå\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "‚úÖ Saved to prepared_data/test_2_1_1.csv | Speed: 21916.49 seq/s\n",
      "\n",
      "üöÄ Processing: test_4_1_1.csv (52776 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52776/52776 [00:01<00:00, 34684.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type       | Motif at Expected Index   | Status\n",
      "------------------------------------------------------------\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [TA] | ‚ùå\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "‚úÖ Saved to prepared_data/test_4_1_1.csv | Speed: 22834.42 seq/s\n",
      "\n",
      "üöÄ Processing: test_10_1_1.csv (105708 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105708/105708 [00:02<00:00, 35271.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type       | Motif at Expected Index   | Status\n",
      "------------------------------------------------------------\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "Acceptor   | Index 600: [TA] | ‚ùå\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "‚úÖ Saved to prepared_data/test_10_1_1.csv | Speed: 22598.00 seq/s\n",
      "\n",
      "üöÄ Processing: test_data.csv (938297 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938297/938297 [00:26<00:00, 35630.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type       | Motif at Expected Index   | Status\n",
      "------------------------------------------------------------\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Donor      | Index 200: [TA] | ‚ùå\n",
      "Donor      | Index 200: [TG] | ‚ùå\n",
      "Donor      | Index 200: [GT] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [AG] | ‚úÖ\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "Acceptor   | Index 600: [CA] | ‚ùå\n",
      "‚úÖ Saved to prepared_data/test_data.csv | Speed: 22851.02 seq/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(PREPARED_FOLDER, exist_ok=True)\n",
    "prepare_csv_datasets(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a75a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donor 0: V·ªã tr√≠ trung t√¢m l√† 'GT' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\n",
      "Donor 1: V·ªã tr√≠ trung t√¢m l√† 'GT' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\n",
      "Donor 2: V·ªã tr√≠ trung t√¢m l√† 'GT' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\n",
      "Donor 3: V·ªã tr√≠ trung t√¢m l√† 'TA' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\n",
      "Donor 4: V·ªã tr√≠ trung t√¢m l√† 'GT' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\n",
      "Acceptor 0: V·ªã tr√≠ trung t√¢m l√† 'AG' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\n",
      "Acceptor 1: V·ªã tr√≠ trung t√¢m l√† 'AG' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\n",
      "Acceptor 2: V·ªã tr√≠ trung t√¢m l√† 'AG' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\n",
      "Acceptor 3: V·ªã tr√≠ trung t√¢m l√† 'CA' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\n",
      "Acceptor 4: V·ªã tr√≠ trung t√¢m l√† 'AG' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\n",
      "Type       | Window (-5 to +5 around center) | Found?\n",
      "--------------------------------------------------\n",
      "Donor      | CACAG[GT]CAGT        | ‚úÖ (Target: GT)\n",
      "Donor      | GTTGT[GT]AAGT        | ‚úÖ (Target: GT)\n",
      "Donor      | AAAAG[GT]ACAC        | ‚úÖ (Target: GT)\n",
      "Donor      | TGAAA[GC]CATT        | ‚ùå (Target: GT)\n",
      "Donor      | CTACG[GT]AAGA        | ‚úÖ (Target: GT)\n",
      "Acceptor   | CAAAA[GC]CTTC        | ‚úÖ (Target: AG)\n",
      "Acceptor   | TTTTA[GA]TGGA        | ‚úÖ (Target: AG)\n",
      "Acceptor   | GCCCA[GC]TGCA        | ‚úÖ (Target: AG)\n",
      "Acceptor   | TACAG[AG]ATTA        | ‚úÖ (Target: AG)\n",
      "Acceptor   | TTCAG[CC]TGAA        | ‚úÖ (Target: AG)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\my_project\\Bio_paper\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SPLAM\\prepared_data\\test_1_1_1.csv')\n",
    "# Ki·ªÉm tra 5 m·∫´u Donor ƒë·∫ßu ti√™n\n",
    "donor_samples = df[df['Splicing_types'] == 1]['sequence'].values[:5]\n",
    "for i, seq in enumerate(donor_samples):\n",
    "    # L·∫•y v√πng trung t√¢m (index 5000 l√† nucleotide t·∫°i v·ªã tr√≠ pos)\n",
    "    center = seq[400:402] \n",
    "    print(f\"Donor {i}: V·ªã tr√≠ trung t√¢m l√† '{center}' (K·ª≥ v·ªçng th∆∞·ªùng l√† GT)\")\n",
    "\n",
    "# Ki·ªÉm tra 5 m·∫´u Acceptor ƒë·∫ßu ti√™n\n",
    "acceptor_samples = df[df['Splicing_types'] == 2]['sequence'].values[:5]\n",
    "for i, seq in enumerate(acceptor_samples):\n",
    "    center = seq[398:400]\n",
    "    print(f\"Acceptor {i}: V·ªã tr√≠ trung t√¢m l√† '{center}' (K·ª≥ v·ªçng th∆∞·ªùng l√† AG)\")\n",
    "\n",
    "def diagnose_splice_sites(df, sample_size=5):\n",
    "    print(f\"{'Type':<10} | {'Window (-5 to +5 around center)':<20} | {'Found?'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for label, name in [(1, 'Donor'), (2, 'Acceptor')]:\n",
    "        samples = df[df['Splicing_types'] == label].sample(min(sample_size, len(df)))\n",
    "        for _, row in samples.iterrows():\n",
    "            seq = row['sequence']\n",
    "            # L·∫•y 11 nucleotide (v·ªã tr√≠ 5000 l√† trung t√¢m)\n",
    "            window = seq[395:406]\n",
    "            \n",
    "            target = \"GT\" if label == 1 else \"AG\"\n",
    "            found = \"‚úÖ\" if target in window else \"‚ùå\"\n",
    "            \n",
    "            # Highlight v·ªã tr√≠ trung t√¢m b·∫±ng d·∫•u ngo·∫∑c []\n",
    "            display_win = window[:5] + \"[\" + window[5:7] + \"]\" + window[7:]\n",
    "            print(f\"{name:<10} | {display_win:<20} | {found} (Target: {target})\")\n",
    "\n",
    "# Ch·∫°y sau khi load df t·ª´ prepared_data\n",
    "diagnose_splice_sites(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03407f29",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64f1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SPLAM Model on cuda...\n",
      "Successfully loaded SPLAM model.\n"
     ]
    }
   ],
   "source": [
    "# Load model SPLAM (TorchScript)\n",
    "print(f\"Loading SPLAM Model on {device}...\")\n",
    "model = torch.jit.load(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Successfully loaded SPLAM model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0de321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [CHECKING ALIGNMENT] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  20%|‚ñà‚ñà        | 1/5 [00:15<01:03, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_1_1_1.csv processed. Speed: 1669.35 seq/s\n",
      "\n",
      "--- [CHECKING ALIGNMENT] ---\n",
      "‚úÖ M·∫´u 14288 (Donor): Motif t·∫°i 200: TA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:35<00:54, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_2_1_1.csv processed. Speed: 1788.54 seq/s\n",
      "\n",
      "--- [CHECKING ALIGNMENT] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:03<00:45, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_4_1_1.csv processed. Speed: 1917.52 seq/s\n",
      "\n",
      "--- [CHECKING ALIGNMENT] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:07<00:44, 22.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_tensor) \u001b[38;5;66;03m# (Batch, 3, 800)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Chuy·ªÉn Logits sang Probs (Softmax tr√™n chi·ªÅu l·ªõp dim=1)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# L·∫•y x√°c su·∫•t c·ªßa c·∫£ 3 l·ªõp t·∫°i ƒê√öNG v·ªã tr√≠ junction (index 400)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# SPLAM: 0=Null, 1=Acceptor, 2=Donor\u001b[39;00m\n\u001b[0;32m     64\u001b[0m p_null \u001b[38;5;241m=\u001b[39m probs[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m400\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def one_hot_encode_splam(seq):\n",
    "    \"\"\"\n",
    "    M√£ h√≥a chu·ªói DNA th√†nh ma tr·∫≠n (4, 800)\n",
    "    Th·ª© t·ª± c√°c h√†ng: 0:A, 1:C, 2:G, 3:T\n",
    "    \"\"\"\n",
    "    seq = seq.upper()\n",
    "    seq_code = np.zeros((4, len(seq)), dtype=np.float32)\n",
    "    \n",
    "    # S·ª≠ d·ª•ng numpy vectorization ƒë·ªÉ tƒÉng t·ªëc\n",
    "    sn = np.frombuffer(seq.encode('ascii'), dtype=np.int8)\n",
    "    seq_code[0, sn == ord('A')] = 1.0\n",
    "    seq_code[1, sn == ord('C')] = 1.0\n",
    "    seq_code[2, sn == ord('G')] = 1.0\n",
    "    seq_code[3, sn == ord('T')] = 1.0\n",
    "    \n",
    "    # M·ªçi k√Ω t·ª± kh√°c (N, R, Y...) s·∫Ω ƒë·ªÉ l√† vector [0, 0, 0, 0]\n",
    "    return seq_code\n",
    "\n",
    "def debug_one_hot_splam(sequences, labels, sample_size=1):\n",
    "    print(\"\\n--- [CHECKING ALIGNMENT] ---\")\n",
    "    indices = np.random.choice(len(sequences), sample_size)\n",
    "    for idx in indices:\n",
    "        seq = sequences[idx]\n",
    "        label = labels[idx]\n",
    "        if label == 1: # Donor\n",
    "            print(f\"‚úÖ M·∫´u {idx} (Donor): Motif t·∫°i 200: {seq[400:402]}\")\n",
    "        elif label == 2: # Acceptor\n",
    "            print(f\"‚úÖ M·∫´u {idx} (Acceptor): Motif t·∫°i 598:600: {seq[398:400]}\")\n",
    "\n",
    "def get_batches(sequences, batch_size):\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch_seqs = sequences[i:i + batch_size]\n",
    "        # Encode v√† stack th√†nh tensor (Batch, 4, 800)\n",
    "        batch_data = np.array([one_hot_encode_splam(s) for s in batch_seqs])\n",
    "        yield torch.tensor(batch_data, dtype=torch.float32).to(device)\n",
    "\n",
    "pbar_files = tqdm(test_files, desc=\"Overall Progress\")\n",
    "\n",
    "for csv_file in pbar_files:\n",
    "    path = os.path.join(PREPARED_FOLDER, csv_file)\n",
    "    if not os.path.exists(path): continue\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    y_true = df['Splicing_types'].values\n",
    "    debug_one_hot_splam(df['sequence'].values,labels=y_true)\n",
    "    sequences = df['sequence'].values\n",
    "    \n",
    "    all_probs = []\n",
    "    inference_start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_tensor in tqdm(get_batches(sequences, BATCH_SIZE), \n",
    "                                total=int(np.ceil(len(sequences)/BATCH_SIZE)), \n",
    "                                desc=\" ‚Ü™ Inferencing\", leave=False):\n",
    "            outputs = model(batch_tensor) # (Batch, 3, 800)\n",
    "        \n",
    "            # Chuy·ªÉn Logits sang Probs (Softmax tr√™n chi·ªÅu l·ªõp dim=1)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # L·∫•y x√°c su·∫•t c·ªßa c·∫£ 3 l·ªõp t·∫°i ƒê√öNG v·ªã tr√≠ junction (index 400)\n",
    "            # SPLAM: 0=Null, 1=Acceptor, 2=Donor\n",
    "            p_null = probs[:, 0, 400]\n",
    "            p_acc  = probs[:, 1, 400]\n",
    "            p_don  = probs[:, 2, 400]\n",
    "\n",
    "            # Stack theo ƒë√∫ng th·ª© t·ª± nh√£n c·ªßa b·∫°n: 0: Null, 1: Donor, 2: Acceptor\n",
    "            batch_probs = np.stack([p_null, p_don, p_acc], axis=1)\n",
    "            all_probs.append(batch_probs)\n",
    "\n",
    "    # --- SAU V√íNG L·∫∂P ---\n",
    "    avg_probs = np.vstack(all_probs)\n",
    "    preds_mapped = np.argmax(avg_probs, axis=1) # D√πng argmax b√¨nh th∆∞·ªùng\n",
    "    \n",
    "    total_duration = time.time() - inference_start\n",
    "\n",
    "    # --- L∆ØU K·∫æT QU·∫¢ ---\n",
    "    try:\n",
    "        from metrics import compute_metrics \n",
    "        # S·ª≠ d·ª•ng final_probs ƒë·ªÉ t√≠nh AUC/AUPRC\n",
    "        results = compute_metrics(y_true, preds_mapped, probs=avg_probs)\n",
    "        \n",
    "        results['benchmarking'] = {\n",
    "            'total_inference_time_sec': round(total_duration, 4),\n",
    "            'samples_per_second': round(len(df) / total_duration, 2)\n",
    "        }\n",
    "        \n",
    "        cm = confusion_matrix(y_true, preds_mapped)\n",
    "        output = {\"metrics\": results, \"confusion_matrix\": cm.tolist()}\n",
    "        \n",
    "        json_path = os.path.join(RESULTS_DIR, csv_file.replace('.csv', '_results.json'))\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(output, f, indent=4)\n",
    "            \n",
    "        print(f\"‚úÖ {csv_file} processed. Speed: {results['benchmarking']['samples_per_second']} seq/s\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error with {csv_file}: {e}\")\n",
    "\n",
    "print(\"\\n‚ú® SPLAM INFERENCE COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- M·∫´u Donor 0 ---\n",
      "L·ªõp 1 (Acc) t·∫°i 200: 0.2119 | L·ªõp 2 (Don) t·∫°i 200: 0.2120\n",
      "L·ªõp 1 (Acc) t·∫°i 600: 0.2121 | L·ªõp 2 (Don) t·∫°i 600: 0.2120\n",
      "--- M·∫´u Donor 5 ---\n",
      "L·ªõp 1 (Acc) t·∫°i 200: 0.2122 | L·ªõp 2 (Don) t·∫°i 200: 0.2128\n",
      "L·ªõp 1 (Acc) t·∫°i 600: 0.2123 | L·ªõp 2 (Don) t·∫°i 600: 0.2120\n",
      "--- M·∫´u Donor 9 ---\n",
      "L·ªõp 1 (Acc) t·∫°i 200: 0.2119 | L·ªõp 2 (Don) t·∫°i 200: 0.2119\n",
      "L·ªõp 1 (Acc) t·∫°i 600: 0.2119 | L·ªõp 2 (Don) t·∫°i 600: 0.2119\n",
      "--- M·∫´u Donor 15 ---\n",
      "L·ªõp 1 (Acc) t·∫°i 200: 0.2119 | L·ªõp 2 (Don) t·∫°i 200: 0.2119\n",
      "L·ªõp 1 (Acc) t·∫°i 600: 0.2119 | L·ªõp 2 (Don) t·∫°i 600: 0.2119\n",
      "--- M·∫´u Donor 16 ---\n",
      "L·ªõp 1 (Acc) t·∫°i 200: 0.2119 | L·ªõp 2 (Don) t·∫°i 200: 0.2119\n",
      "L·ªõp 1 (Acc) t·∫°i 600: 0.2119 | L·ªõp 2 (Don) t·∫°i 600: 0.2119\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y th·ª≠ cho 5 m·∫´u Donor (y_true == 1)\n",
    "test_idx = df[df['Splicing_types'] == 1].index[:5]\n",
    "for idx in test_idx:\n",
    "    seq = df.iloc[idx]['sequence']\n",
    "    input_t = torch.tensor(one_hot_encode_splam(seq)).unsqueeze(0).to(device)\n",
    "    out = torch.softmax(model(input_t), dim=1)[0].cpu().detach().numpy()\n",
    "    \n",
    "    print(f\"--- M·∫´u Donor {idx} ---\")\n",
    "    print(f\"L·ªõp 1 (Acc) t·∫°i 200: {out[1, 200]:.4f} | L·ªõp 2 (Don) t·∫°i 200: {out[2, 200]:.4f}\")\n",
    "    print(f\"L·ªõp 1 (Acc) t·∫°i 600: {out[1, 600]:.4f} | L·ªõp 2 (Don) t·∫°i 600: {out[2, 600]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e32dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y ƒëo·∫°n n√†y trong Notebook c·ªßa b·∫°n\n",
    "with torch.no_grad():\n",
    "    sample_don = torch.tensor(one_hot_encode_splam(donor_sequence)).unsqueeze(0).to(device)\n",
    "    output = torch.softmax(model(sample_don), dim=1)[0].cpu().numpy() # (3, 800)\n",
    "    \n",
    "    # T√¨m v·ªã tr√≠ m√† l·ªõp Donor (2) ƒë·∫°t Max\n",
    "    peak_pos = np.argmax(output[2, :])\n",
    "    print(f\"Donor Peak t√¨m th·∫•y t·∫°i index: {peak_pos} v·ªõi x√°c su·∫•t {output[2, peak_pos]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
