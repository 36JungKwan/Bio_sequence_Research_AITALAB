{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2881d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training on: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import multimolecule\n",
    "# --- IMPORT MODULE C·ª¶A B·∫†N ---\n",
    "from metrics import compute_metrics\n",
    "\n",
    "# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (B·∫°n s·ª≠a ·ªü ƒë√¢y) ---\n",
    "BASE_PATH = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\"\n",
    "DATA_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\data\"      # Folder ch·ª©a data input\n",
    "RESULT_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\"                      # Folder l∆∞u k·∫øt qu·∫£ JSON\n",
    "OUTPUT_MODEL_DIR = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\pretrained_model\"             # Folder ch·ª©a c√°c model .h5\n",
    "\n",
    "# S·ª≠a d√≤ng n√†y:\n",
    "BASE_MODEL_NAME = \"multimolecule/splicebert-human.510\"\n",
    "\n",
    "# Training Config\n",
    "BATCH_SIZE = 16       \n",
    "GRAD_ACCUMULATION = 1 \n",
    "EPOCHS = 3           \n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Training on: {DEVICE}\")\n",
    "\n",
    "# Class h·ªó tr·ª£ l∆∞u JSON kh√¥ng b·ªã l·ªói Numpy\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        if isinstance(obj, np.floating): return float(obj)\n",
    "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a412c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpliceTrainDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def prepare_dataset(file_path, tokenizer):\n",
    "    \"\"\"ƒê·ªçc file CSV, chia train/val v√† tokenize\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    sequences = df['sequence'].tolist()\n",
    "    labels = df['Splicing_types'].tolist()\n",
    "    \n",
    "    # Chia 80% Train - 20% Val\n",
    "    train_seqs, val_seqs, train_labels, val_labels = train_test_split(\n",
    "        sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_encodings = tokenizer(train_seqs, truncation=True, padding=True, max_length=512)\n",
    "    val_encodings = tokenizer(val_seqs, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    return SpliceTrainDataset(train_encodings, train_labels), SpliceTrainDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a0e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_wrapper(eval_pred):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi output c·ªßa Trainer (Logits) th√†nh input cho metrics.py (Probs)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Trainer tr·∫£ v·ªÅ numpy array, c·∫ßn d√πng softmax ƒë·ªÉ ra x√°c su·∫•t\n",
    "    # L∆∞u √Ω: logits c√≥ th·ªÉ l√† tuple, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu c·∫ßn\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "        \n",
    "    # Chuy·ªÉn logits -> probabilities (Softmax)\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    preds = np.argmax(probs, axis=-1)\n",
    "    \n",
    "    # G·ªçi h√†m t·ª´ file metrics.py c·ªßa b·∫°n\n",
    "    # H√†m n√†y tr·∫£ v·ªÅ dict {'accuracy':..., 'f1':..., 'auc':...}\n",
    "    return compute_metrics(labels, preds, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed53db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(data_filename, output_subfolder_name):\n",
    "    print(f\"\\n{'='*20} TRAINING: {output_subfolder_name} {'='*20}\")\n",
    "    \n",
    "    input_file = os.path.join(DATA_FOLDER, data_filename)\n",
    "    save_model_path = os.path.join(OUTPUT_MODEL_DIR, output_subfolder_name)\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y data: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # 1. Load Tokenizer & Model\n",
    "    print(f\"‚è≥ Loading {BASE_MODEL_NAME}...\")\n",
    "    try:\n",
    "        # Load Tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "        \n",
    "        # Load Model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            BASE_MODEL_NAME, \n",
    "            num_labels=3,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói t·∫£i model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Prepare Data\n",
    "    print(\"‚è≥ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...\")\n",
    "    try:\n",
    "        train_dataset, val_dataset = prepare_dataset(input_file, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 3. Setup Trainer\n",
    "    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY: Thay evaluation_strategy b·∫±ng eval_strategy ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./checkpoints/{output_subfolder_name}\",\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "        fp16=False,               \n",
    "        bf16=True,                  \n",
    "        tf32=True,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",        # <--- D√íNG ƒê√É S·ª¨A (Phi√™n b·∫£n m·ªõi d√πng eval_strategy)\n",
    "        save_strategy=\"epoch\",        \n",
    "        load_best_model_at_end=True,  \n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        dataloader_num_workers=0 \n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_wrapper, \n",
    "    )\n",
    "    \n",
    "    # 4. Start Training\n",
    "    print(\"üöÄ B·∫Øt ƒë·∫ßu Train...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 5. L∆∞u Model Final\n",
    "    print(f\"üíæ ƒêang l∆∞u model v√†o: {save_model_path}\")\n",
    "    model.save_pretrained(save_model_path)\n",
    "    tokenizer.save_pretrained(save_model_path)\n",
    "    \n",
    "    # 6. ƒê√°nh gi√° & Ghi JSON\n",
    "    print(\"üìä ƒêang t√≠nh to√°n metrics cu·ªëi c√πng...\")\n",
    "    final_metrics = trainer.evaluate()\n",
    "    \n",
    "    output_json_path = os.path.join(RESULT_FOLDER, f\"train_result_{output_subfolder_name}.json\")\n",
    "    \n",
    "    final_output = {\n",
    "        \"meta_data\": {\n",
    "            \"task\": \"training_validation\",\n",
    "            \"model_name\": output_subfolder_name,\n",
    "            \"source_data\": data_filename,\n",
    "            \"base_model\": BASE_MODEL_NAME\n",
    "        },\n",
    "        \"metrics\": final_metrics\n",
    "    }\n",
    "    \n",
    "    os.makedirs(RESULT_FOLDER, exist_ok=True)\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(final_output, f, cls=NpEncoder, indent=4)\n",
    "        \n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ metrics v√†o: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fe7505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH KI·ªÇM TH·ª¨...\n",
      "\n",
      "==================== TRAINING: SpliceBERT_ratio_1_1_1 ====================\n",
      "‚è≥ Loading multimolecule/splicebert-human.510...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02babc8723a45ad880d4050f16c79cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\multimolecule\\modules\\criterions\\generic.py:61: UserWarning: `problem_type` is not set. Assuming multiclass. \n",
      "This can lead to unexpected behavior. Please set `problem_type` explicitly.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1317' max='3948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1317/3948 00:38 < 01:17, 33.78 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/329 03:16 < 03:23, 0.82 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH KI·ªÇM TH·ª¨...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_file, model_name \u001b[38;5;129;01min\u001b[39;00m tasks:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müèÅ HO√ÄN T·∫§T TO√ÄN B·ªò!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(data_filename, output_subfolder_name)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 4. Start Training\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ B·∫Øt ƒë·∫ßu Train...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 5. L∆∞u Model Final\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ ƒêang l∆∞u model v√†o: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:2170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2168\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:2642\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2642\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2648\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:3021\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3019\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3021\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3022\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:2970\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2970\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2971\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2973\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:4290\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4286\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n\u001b[0;32m   4288\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 4290\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4291\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4293\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model_preparation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer.py:4505\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4503\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function(logits)\n\u001b[0;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4505\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4507\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function(labels)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:315\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:131\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[0;32m    134\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    135\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dung\\anaconda3\\envs\\splice_env\\lib\\site-packages\\transformers\\trainer_pt_utils.py:89\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[1;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[0;32m     86\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[0;32m     92\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Danh s√°ch c√°c c·∫∑p (File Data, T√™n Model t∆∞∆°ng ·ª©ng trong folder pretrained_model)\n",
    "    # V√≠ d·ª•: Data test t·ªâ l·ªá 1:1:1 ch·∫°y v·ªõi model train tr√™n t·ªâ l·ªá 1:1:1\n",
    "    \n",
    "    tasks = [\n",
    "        # (\"T√™n_file_data.csv\", \"T√™n_folder_ho·∫∑c_file_model\")\n",
    "        (\"test_1_1_1.csv\", \"SpliceBERT_ratio_1_1_1\"), \n",
    "        (\"test_2_1_1.csv\", \"SpliceBERT_ratio_2_1_1\"),\n",
    "        (\"test_4_1_1.csv\", \"SpliceBERT_ratio_4_1_1\"),\n",
    "        (\"test_10_1_1.csv\", \"SpliceBERT_ratio_10_1_1\"),\n",
    "        (\"test_data.csv\", \"SpliceBERT_ratio_100_1_1\")\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ B·∫ÆT ƒê·∫¶U QU√Å TR√åNH KI·ªÇM TH·ª¨...\")\n",
    "    \n",
    "    for data_file, model_name in tasks:\n",
    "        run_training(data_file, model_name)\n",
    "        \n",
    "    print(\"\\nüèÅ HO√ÄN T·∫§T TO√ÄN B·ªò!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d09d2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Hardware: NVIDIA GeForce RTX 5080 | CUDA: 12.8\n",
      "üé¨ B·∫ÆT ƒê·∫¶U CH·∫†Y INFERENCE H√ÄNG LO·∫†T...\n",
      "\n",
      "==================== PROCESSING: test_1_1_1.csv ====================\n",
      "‚è≥ Loading Model: multimolecule/splicebert-human.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eea8ce1b8d14416918992c8b76b1280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n 26310 m·∫´u...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa86e69d891c40599d16fc00ff43e5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Th·ªùi gian: 89.62s (294 seq/s)\n",
      "üìä ƒêang t√≠nh metrics...\n",
      "Warning: Could not compute probability metrics: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\\result_test_1_1_1_on_splicebert-human.510.json\n",
      "   Accuracy: 0.3423\n",
      "\n",
      "==================== PROCESSING: test_2_1_1.csv ====================\n",
      "‚è≥ Loading Model: multimolecule/splicebert-human.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bb229a16714e5fa70b88c736eae40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n 35132 m·∫´u...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1793ce64e6e94f0498432a00d8283cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Th·ªùi gian: 113.20s (310 seq/s)\n",
      "üìä ƒêang t√≠nh metrics...\n",
      "Warning: Could not compute probability metrics: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\\result_test_2_1_1_on_splicebert-human.510.json\n",
      "   Accuracy: 0.3165\n",
      "\n",
      "==================== PROCESSING: test_4_1_1.csv ====================\n",
      "‚è≥ Loading Model: multimolecule/splicebert-human.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80b9076e0b64fab9267858c0b1106ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n 52776 m·∫´u...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965ad9aad0540289087b7f11469d1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Th·ªùi gian: 165.15s (320 seq/s)\n",
      "üìä ƒêang t√≠nh metrics...\n",
      "Warning: Could not compute probability metrics: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\\result_test_4_1_1_on_splicebert-human.510.json\n",
      "   Accuracy: 0.2049\n",
      "\n",
      "==================== PROCESSING: test_10_1_1.csv ====================\n",
      "‚è≥ Loading Model: multimolecule/splicebert-human.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f196ddc89942e5ae1f0c3ea824fe86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n 105708 m·∫´u...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ad2dc9050646418200166567ea8b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Th·ªùi gian: 339.60s (311 seq/s)\n",
      "üìä ƒêang t√≠nh metrics...\n",
      "Warning: Could not compute probability metrics: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\\result_test_10_1_1_on_splicebert-human.510.json\n",
      "   Accuracy: 0.1552\n",
      "\n",
      "==================== PROCESSING: test_data.csv ====================\n",
      "‚è≥ Loading Model: multimolecule/splicebert-human.510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f426bbba1ddb4a3b926eb25138718867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSpliceBertForSequencePrediction LOAD REPORT\u001b[0m from: multimolecule/splicebert-human.510\n",
      "Key                                 | Status     | \n",
      "------------------------------------+------------+-\n",
      "lm_head.transform.layer_norm.bias   | UNEXPECTED | \n",
      "lm_head.transform.layer_norm.weight | UNEXPECTED | \n",
      "lm_head.decoder.weight              | UNEXPECTED | \n",
      "lm_head.bias                        | UNEXPECTED | \n",
      "lm_head.transform.dense.bias        | UNEXPECTED | \n",
      "lm_head.transform.dense.weight      | UNEXPECTED | \n",
      "model.pooler.dense.weight           | MISSING    | \n",
      "model.pooler.dense.bias             | MISSING    | \n",
      "sequence_head.decoder.weight        | MISSING    | \n",
      "sequence_head.decoder.bias          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n 938297 m·∫´u...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e36cfc225745f39d534907a1539c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3666 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Th·ªùi gian: 3047.05s (308 seq/s)\n",
      "üìä ƒêang t√≠nh metrics...\n",
      "Warning: Could not compute probability metrics: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes\n",
      "‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\\result_test_data_on_splicebert-human.510.json\n",
      "   Accuracy: 0.0120\n",
      "\n",
      "üèÅ HO√ÄN T·∫§T!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import multimolecule  # Import ƒë·ªÉ nh·∫≠n di·ªán SpliceBERT\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- Import file metrics c·ªßa b·∫°n ---\n",
    "# ƒê·∫£m b·∫£o file metrics.py n·∫±m c√πng th∆∞ m·ª•c v·ªõi file notebook n√†y\n",
    "from metrics import compute_metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. C·∫§U H√åNH (B·∫†N CH·ªà C·∫¶N S·ª¨A ·ªû ƒê√ÇY)\n",
    "# ==============================================================================\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n folder ch·ª©a c√°c file CSV ƒë√£ x·ª≠ l√Ω (ƒë√£ crop 510bp)\n",
    "DATA_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\data\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n folder ƒë·ªÉ l∆∞u k·∫øt qu·∫£ JSON\n",
    "RESULT_FOLDER = r\"D:\\Bio_sequence_Research_AITALAB\\benchmark\\task1_splicing_prediction\\SpliceBERT\\result\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n Model b·∫°n mu·ªën test\n",
    "# - N·∫øu l√† model tr√™n m·∫°ng: ƒë·ªÉ t√™n (vd: \"multimolecule/splicebert-human.510\")\n",
    "# - N·∫øu l√† model trong m√°y: ƒëi·ªÅn ƒë∆∞·ªùng d·∫´n folder (vd: r\"D:\\Study\\...\\pretrained_model\\SpliceBERT_ratio_1_1_1\")\n",
    "MODEL_PATH = \"multimolecule/splicebert-human.510\"  \n",
    "\n",
    "BIAS_ADJUSTMENT = None\n",
    "\n",
    "# C·∫•u h√¨nh ph·∫ßn c·ª©ng cho RTX 5080\n",
    "BATCH_SIZE = 256        # Inference r·∫•t nh·∫π, 5080 c√≥ th·ªÉ g√°nh 256-512 m·∫´u/l·∫ßn\n",
    "USE_BF16 = True         # B·∫≠t BFloat16 (Nhanh & Chu·∫©n tr√™n RTX 50-series)\n",
    "COMPILE_MODEL = False    # B·∫≠t torch.compile (TƒÉng t·ªëc c·ª±c m·∫°nh tr√™n PyTorch 2.0+)\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# Setup thi·∫øt b·ªã\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Hardware: {torch.cuda.get_device_name(0)} | CUDA: {torch.version.cuda}\")\n",
    "\n",
    "# Class Dataset (Gi·∫£n l∆∞·ª£c cho Inference)\n",
    "class SpliceInferenceDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_len=512):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gi·∫£ ƒë·ªãnh data ƒë√£ crop chu·∫©n 510bp\n",
    "        seq = str(self.df.iloc[idx]['sequence'])\n",
    "        label = int(self.df.iloc[idx]['Splicing_types'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            seq,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Class h·ªó tr·ª£ l∆∞u JSON\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer): return int(obj)\n",
    "        if isinstance(obj, np.floating): return float(obj)\n",
    "        if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def run_inference(data_filename):\n",
    "    print(f\"\\n{'='*20} PROCESSING: {data_filename} {'='*20}\")\n",
    "    \n",
    "    file_path = os.path.join(DATA_FOLDER, data_filename)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # 1. Load Model & Tokenizer\n",
    "    print(f\"‚è≥ Loading Model: {MODEL_PATH}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_PATH, \n",
    "            num_labels=3,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.to(DEVICE)\n",
    "        model.eval() # Ch·∫ø ƒë·ªô ƒë√°nh gi√° (kh√¥ng train)\n",
    "        \n",
    "        # T·ªëi ∆∞u h√≥a Model (Optional)\n",
    "        if COMPILE_MODEL:\n",
    "            print(\"üöÄ Compiling model for RTX 5080...\")\n",
    "            try:\n",
    "                model = torch.compile(model)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Compile th·∫•t b·∫°i (v·∫´n ch·∫°y b√¨nh th∆∞·ªùng): {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói load model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare Data\n",
    "    dataset = SpliceInferenceDataset(file_path, tokenizer)\n",
    "    # num_workers=0 ƒë·ªÉ tr√°nh l·ªói tr√™n Windows\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # 3. Inference Loop\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(f\"üöÄ ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n {len(dataset)} m·∫´u...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # D√πng torch.autocast ƒë·ªÉ ch·∫°y BF16 (nhanh h∆°n FP32)\n",
    "    dtype = torch.bfloat16 if USE_BF16 and torch.cuda.is_bf16_supported() else torch.float32\n",
    "    \n",
    "    with torch.no_grad(): # T·∫Øt t√≠nh to√°n ƒë·∫°o h√†m (Ti·∫øt ki·ªám VRAM)\n",
    "        for batch in tqdm(loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().float().numpy())\n",
    "            all_labels.extend(labels.cpu().float().numpy())\n",
    "            all_probs.extend(probs.cpu().float().numpy())\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è Th·ªùi gian: {inference_time:.2f}s ({len(dataset)/inference_time:.0f} seq/s)\")\n",
    "\n",
    "# 4. T√≠nh Metrics & Confusion Matrix\n",
    "    print(\"üìä ƒêang t√≠nh metrics...\")\n",
    "    try:\n",
    "        # T√≠nh c√°c ch·ªâ s·ªë c∆° b·∫£n (Accuracy, F1, AUC...)\n",
    "        metric_results = compute_metrics(all_labels, all_preds, probs=np.array(all_probs))\n",
    "        \n",
    "        # T√≠nh Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        # X·ª≠ l√Ω Ratio t·ª´ t√™n file (V√≠ d·ª•: test_1_1_1.csv -> 1-1-1)\n",
    "        # Logic: B·ªè ƒëu√¥i .csv, b·ªè ti·ªÅn t·ªë \"test_\", thay _ b·∫±ng -\n",
    "        ratio_str = data_filename.replace(\".csv\", \"\").replace(\"test_\", \"\").replace(\"_\", \"-\")\n",
    "        \n",
    "        # T·∫°o c·∫•u tr√∫c JSON ph·∫≥ng nh∆∞ y√™u c·∫ßu\n",
    "        final_output = metric_results.copy() # Copy c√°c metrics v√†o level ngo√†i c√πng\n",
    "        \n",
    "        final_output[\"confusion_matrix\"] = cm.tolist() # Th√™m ma tr·∫≠n nh·∫ßm l·∫´n\n",
    "        final_output[\"meta\"] = {\n",
    "            \"bias_applied\": BIAS_ADJUSTMENT if BIAS_ADJUSTMENT else [0.0, 0.0, 0.0],\n",
    "            \"ratio\": ratio_str\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói t√≠nh metrics: {e}\")\n",
    "        return\n",
    "\n",
    "    # 5. L∆∞u k·∫øt qu·∫£\n",
    "    model_name_clean = os.path.basename(MODEL_PATH).replace(\":\", \"_\").replace(\"/\", \"_\")\n",
    "    output_filename = f\"result_{data_filename.replace('.csv', '')}_on_{model_name_clean}.json\"\n",
    "    output_path = os.path.join(RESULT_FOLDER, output_filename)\n",
    "    \n",
    "    os.makedirs(RESULT_FOLDER, exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(final_output, f, cls=NpEncoder, indent=4)\n",
    "        \n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ chu·∫©n format t·∫°i: {output_path}\")\n",
    "    print(f\"   Accuracy: {final_output.get('accuracy', 0):.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CH·∫†Y CH∆Ø∆†NG TR√åNH\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Danh s√°ch c√°c file c·∫ßn ch·∫°y\n",
    "    files_to_test = [\n",
    "        \"test_1_1_1.csv\", \n",
    "        \"test_2_1_1.csv\",\n",
    "        \"test_4_1_1.csv\",\n",
    "        \"test_10_1_1.csv\",\n",
    "        \"test_data.csv\" # File m·ªõi c·ªßa b·∫°n\n",
    "    ]\n",
    "    \n",
    "    print(\"üé¨ B·∫ÆT ƒê·∫¶U CH·∫†Y INFERENCE H√ÄNG LO·∫†T...\")\n",
    "    for f in files_to_test:\n",
    "        run_inference(f)\n",
    "    print(\"\\nüèÅ HO√ÄN T·∫§T!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
