{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec187f33",
   "metadata": {
    "id": "ec187f33"
   },
   "source": [
    "DATA PREPARATION - EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3649bd9a",
   "metadata": {
    "id": "3649bd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: pre_train_splicing_prediction.csv (25325376 rows)\n",
      "\n",
      "--- DONE ---\n",
      "Train_val set size: 24387079 rows.\n",
      "Test set size: 938297 rows.\n",
      "\n",
      "--- 'CHROM' ---\n",
      "\n",
      "Train_val set distribution:\n",
      "CHROM\n",
      "chr1     0.1038\n",
      "chr2     0.0805\n",
      "chr3     0.0677\n",
      "chr11    0.0606\n",
      "chr19    0.0602\n",
      "chr17    0.0597\n",
      "chr12    0.0541\n",
      "chr7     0.0501\n",
      "chr10    0.0470\n",
      "chr6     0.0453\n",
      "chr16    0.0452\n",
      "chr9     0.0442\n",
      "chr5     0.0423\n",
      "chr4     0.0410\n",
      "chr8     0.0350\n",
      "chrX     0.0339\n",
      "chr15    0.0332\n",
      "chr14    0.0331\n",
      "chr22    0.0266\n",
      "chr13    0.0185\n",
      "chr18    0.0157\n",
      "chrY     0.0022\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set distribution:\n",
      "CHROM\n",
      "chr20    0.721\n",
      "chr21    0.279\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- 'Splicing_types' ---\n",
      "Original distribution:\n",
      "Splicing_types\n",
      "0    0.9804\n",
      "1    0.0099\n",
      "2    0.0097\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Train_val set distribution:\n",
      "Splicing_types\n",
      "0    0.9804\n",
      "1    0.0099\n",
      "2    0.0097\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set distribution:\n",
      "Splicing_types\n",
      "0    0.9814\n",
      "1    0.0094\n",
      "2    0.0092\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Train_val set saved: raw/train_val_data.csv\n",
      "\n",
      "Test set saved: raw/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "##### Run this cell one time only\n",
    "from train_test_split import train_test_set_split\n",
    "\n",
    "FILE_PATH = r'pre_train_splicing_prediction.csv'\n",
    "CHROM_COLUMN = 'CHROM'\n",
    "TEST_CHROMS = ['chr20', 'chr21']\n",
    "SPLICE_COLUMN = 'Splicing_types'\n",
    "\n",
    "train, test = train_test_set_split(FILE_PATH, CHROM_COLUMN, TEST_CHROMS, SPLICE_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cb372",
   "metadata": {},
   "source": [
    "### Downscale class 0 from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff1d65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Splicing_types\n",
       "0    920809\n",
       "1      8822\n",
       "2      8666\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_data.csv')\n",
    "df.value_counts('Splicing_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3d16a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tỉ lệ 10:1:1 -> Tổng số mẫu: 105708\n",
      "Splicing_types\n",
      "0    88220\n",
      "1     8822\n",
      "2     8666\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Tỉ lệ 4:1:1 -> Tổng số mẫu: 52776\n",
      "Splicing_types\n",
      "0    35288\n",
      "1     8822\n",
      "2     8666\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Tỉ lệ 2:1:1 -> Tổng số mẫu: 35132\n",
      "Splicing_types\n",
      "0    17644\n",
      "1     8822\n",
      "2     8666\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Tỉ lệ 1:1:1 -> Tổng số mẫu: 26310\n",
      "Splicing_types\n",
      "0    8822\n",
      "1    8822\n",
      "2    8666\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def undersample_by_ratio(df, label_col='Splicing_types', ratio_0=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Hàm cắt giảm Class 0 theo tỉ lệ mong muốn so với Class 1.\n",
    "    Giữ nguyên Class 1 và Class 2.\n",
    "    \"\"\"\n",
    "    # Tách các nhóm dữ liệu\n",
    "    df_0 = df[df[label_col] == 0]\n",
    "    df_1 = df[df[label_col] == 1]\n",
    "    df_2 = df[df[label_col] == 2]\n",
    "    \n",
    "    # Lấy số lượng của Class 1 làm mốc (8,822)\n",
    "    base_count = len(df_1)\n",
    "    \n",
    "    # Tính số lượng cần lấy cho Class 0\n",
    "    target_n_0 = int(base_count * ratio_0)\n",
    "    \n",
    "    # Thực hiện lấy mẫu ngẫu nhiên cho Class 0\n",
    "    df_0_sampled = df_0.sample(n=min(len(df_0), target_n_0), random_state=random_state)\n",
    "    \n",
    "    # Gộp lại và xáo trộn (shuffle) thứ tự các dòng\n",
    "    df_final = pd.concat([df_0_sampled, df_1, df_2]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "ratios = [10, 4, 2, 1]\n",
    "datasets = {}\n",
    "\n",
    "for r in ratios:\n",
    "    name = f\"df_{r}_1_1\"\n",
    "    datasets[name] = undersample_by_ratio(df, ratio_0=r)\n",
    "    print(f\"Tỉ lệ {r}:1:1 -> Tổng số mẫu: {len(datasets[name])}\")\n",
    "    print(datasets[name]['Splicing_types'].value_counts().sort_index())\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d76bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['df_10_1_1'].to_csv('test_10_1_1.csv', index=False)\n",
    "datasets['df_4_1_1'].to_csv('test_4_1_1.csv', index=False)\n",
    "datasets['df_2_1_1'].to_csv('test_2_1_1.csv', index=False)\n",
    "datasets['df_1_1_1'].to_csv('test_1_1_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bf916",
   "metadata": {},
   "source": [
    "### Ratio splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3135cf1",
   "metadata": {
    "id": "a3135cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available:\n",
      " Class 0: 23907991 \n",
      "Class 1, 242008 \n",
      "Class 2, 237080\n",
      "unit = 242008\n",
      "\n",
      "Generating ratio 9_1_1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Creating dataset 1/10\n",
      "  - Creating dataset 2/10\n",
      "  - Creating dataset 3/10\n",
      "  - Creating dataset 4/10\n",
      "  - Creating dataset 5/10\n",
      "  - Creating dataset 6/10\n",
      "  - Creating dataset 7/10\n",
      "  - Creating dataset 8/10\n",
      "  - Creating dataset 9/10\n",
      "  - Creating dataset 10/10\n",
      "\n",
      "Generating ratio 4_1_1 ...\n",
      "  - Creating dataset 1/10\n",
      "  - Creating dataset 2/10\n",
      "  - Creating dataset 3/10\n",
      "  - Creating dataset 4/10\n",
      "  - Creating dataset 5/10\n",
      "  - Creating dataset 6/10\n",
      "  - Creating dataset 7/10\n",
      "  - Creating dataset 8/10\n",
      "  - Creating dataset 9/10\n",
      "  - Creating dataset 10/10\n",
      "\n",
      "Generating ratio 2_1_1 ...\n",
      "  - Creating dataset 1/10\n",
      "  - Creating dataset 2/10\n",
      "  - Creating dataset 3/10\n",
      "  - Creating dataset 4/10\n",
      "  - Creating dataset 5/10\n",
      "  - Creating dataset 6/10\n",
      "  - Creating dataset 7/10\n",
      "  - Creating dataset 8/10\n",
      "  - Creating dataset 9/10\n",
      "  - Creating dataset 10/10\n",
      "\n",
      "Generating ratio 1_1_1 ...\n",
      "  - Creating dataset 1/10\n",
      "  - Creating dataset 2/10\n",
      "  - Creating dataset 3/10\n",
      "  - Creating dataset 4/10\n",
      "  - Creating dataset 5/10\n",
      "  - Creating dataset 6/10\n",
      "  - Creating dataset 7/10\n",
      "  - Creating dataset 8/10\n",
      "  - Creating dataset 9/10\n",
      "  - Creating dataset 10/10\n",
      "\n",
      "=== Exporting ratio 9_1_1 (10 sets) ===\n",
      "  - Saved: train_val/9_1_1/set_1/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_1/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_2/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_2/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_3/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_3/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_4/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_4/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_5/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_5/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_6/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_6/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_7/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_7/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_8/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_8/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_9/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_9/val.csv    (398574 rows)\n",
      "  - Saved: train_val/9_1_1/set_10/train.csv  (2258586 rows)\n",
      "  - Saved: train_val/9_1_1/set_10/val.csv    (398574 rows)\n",
      "\n",
      "=== Exporting ratio 4_1_1 (10 sets) ===\n",
      "  - Saved: train_val/4_1_1/set_1/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_1/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_2/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_2/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_3/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_3/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_4/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_4/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_5/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_5/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_6/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_6/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_7/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_7/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_8/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_8/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_9/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_9/val.csv    (217068 rows)\n",
      "  - Saved: train_val/4_1_1/set_10/train.csv  (1230052 rows)\n",
      "  - Saved: train_val/4_1_1/set_10/val.csv    (217068 rows)\n",
      "\n",
      "=== Exporting ratio 2_1_1 (10 sets) ===\n",
      "  - Saved: train_val/2_1_1/set_1/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_1/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_2/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_2/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_3/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_3/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_4/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_4/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_5/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_5/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_6/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_6/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_7/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_7/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_8/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_8/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_9/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_9/val.csv    (144466 rows)\n",
      "  - Saved: train_val/2_1_1/set_10/train.csv  (818638 rows)\n",
      "  - Saved: train_val/2_1_1/set_10/val.csv    (144466 rows)\n",
      "\n",
      "=== Exporting ratio 1_1_1 (10 sets) ===\n",
      "  - Saved: train_val/1_1_1/set_1/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_1/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_2/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_2/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_3/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_3/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_4/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_4/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_5/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_5/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_6/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_6/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_7/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_7/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_8/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_8/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_9/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_9/val.csv    (108165 rows)\n",
      "  - Saved: train_val/1_1_1/set_10/train.csv  (612931 rows)\n",
      "  - Saved: train_val/1_1_1/set_10/val.csv    (108165 rows)\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "##### Run this cell one time only\n",
    "import pandas as pd\n",
    "from ratio_split import ratio_splitting\n",
    "\n",
    "VAL_SIZE = 0.15\n",
    "CHROM_COLUMN = 'CHROM'\n",
    "SPLICE_COLUMN = 'Splicing_types'\n",
    "TRAIN_DATA_PATH = r'raw\\train_val_data.csv' # Fill this only if interupted by error (no need to rerun the 1st cell)\n",
    "\n",
    "try:\n",
    "    train\n",
    "except NameError:\n",
    "    train = None\n",
    "if train is None:\n",
    "    train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "ratio_splitting(train, CHROM_COLUMN, SPLICE_COLUMN, VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e8a485",
   "metadata": {
    "id": "90e8a485"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading Nucleotide Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda\n",
      "Root folder = train_val\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_2\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_2\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:35:01<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_2\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_2\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_2\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:59<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_2\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_2\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_2\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_3\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_3\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:34:45<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_3\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_3\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_3\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:44<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_3\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_3\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_3\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_4\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_4\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:34:58<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_4\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_4\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_4\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:56<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_4\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_4\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_4\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_5\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_5\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:35:03<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_5\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_5\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_5\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:57<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_5\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_5\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_5\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_6\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_6\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:36:04<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_6\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_6\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_6\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:57<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_6\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_6\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_6\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_7\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_7\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:35:47<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_7\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_7\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_7\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:53<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_7\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_7\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_7\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_8\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_8\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:34:45<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_8\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_8\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_8\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:45<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_8\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_8\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_8\n",
      "\n",
      "Processing set: train_val\\4_1_1\\set_9\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_9\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1230052 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19220/19220 [1:35:09<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_9\\train_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_9\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\4_1_1\\set_9\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (217068 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3392/3392 [16:57<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\4_1_1\\set_9\\val_embeddings.pt\n",
      "[deleted] train_val\\4_1_1\\set_9\\val.csv\n",
      "Finished embedding set: train_val\\4_1_1\\set_9\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_1\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_1\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:55:37<00:00,  3.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_1\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_1\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_1\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:28<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_1\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_1\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_1\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_10\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_10\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:52:24<00:00,  3.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_10\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_10\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_10\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:27<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_10\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_10\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_10\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_2\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_2\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:52:16<00:00,  3.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_2\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_2\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_2\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:19<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_2\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_2\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_2\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_3\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_3\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:56:12<00:00,  3.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_3\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_3\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_3\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [31:13<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_3\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_3\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_3\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_4\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_4\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:57:19<00:00,  3.32it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_4\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_4\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_4\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [31:18<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_4\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_4\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_4\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_5\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_5\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:53:21<00:00,  3.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_5\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_5\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_5\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [31:10<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_5\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_5\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_5\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_6\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_6\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:55:28<00:00,  3.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_6\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_6\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_6\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:23<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_6\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_6\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_6\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_7\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_7\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:55:17<00:00,  3.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_7\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_7\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_7\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:47<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_7\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_7\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_7\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_8\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_8\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:55:59<00:00,  3.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_8\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_8\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_8\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:43<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_8\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_8\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_8\n",
      "\n",
      "Processing set: train_val\\9_1_1\\set_9\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_9\\train.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (2258586 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35291/35291 [2:53:44<00:00,  3.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_9\\train_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_9\\train.csv\n",
      "\n",
      "Reading CSV: train_val\\9_1_1\\set_9\\val.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (398574 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6228/6228 [30:57<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] train_val\\9_1_1\\set_9\\val_embeddings.pt\n",
      "[deleted] train_val\\9_1_1\\set_9\\val.csv\n",
      "Finished embedding set: train_val\\9_1_1\\set_9\n",
      "\n",
      "[DONE] All embeddings generated.\n"
     ]
    }
   ],
   "source": [
    "##### Run this cell one time only\n",
    "from extract_embed import embed_train_val_folder\n",
    "\n",
    "DATA_PATH = r\"train_val\"\n",
    "\n",
    "embed_train_val_folder(root=DATA_PATH, replace=True) #Automately delete csv files after embedding -> change to False to keep csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ec5ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\raw\\test_data.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (938297 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14661/14661 [1:14:47<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\raw\\test_data_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\raw\\\\test_data_embeddings.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\raw\\test_data.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e79d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_1_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (26310 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412/412 [02:02<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_1_1_1_embeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\test_1_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_1_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b009b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_2_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (35132 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [02:39<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_2_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\test_2_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_2_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517012ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_4_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (52776 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 825/825 [04:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_4_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\test_4_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_4_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8815c53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_10_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (105708 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [07:58<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_10_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\test_10_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\test_10_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76c0771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_1_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (41283 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 646/646 [03:19<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_1_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\gtex_test_1_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_1_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345286fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_2_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (55059 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 861/861 [04:22<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_2_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\gtex_test_2_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_2_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba22311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_4_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (82611 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1291/1291 [06:33<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_4_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\gtex_test_4_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_4_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27a0115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_10_1_1.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (165267 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2583/2583 [13:06<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_10_1_1_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\gtex_test_10_1_1_embeddings.pt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_10_1_1.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c18378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CSV: D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_data.csv\n",
      "Extracting embeddings from Nucleotide Transformer... (1402857 sequences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5480/5480 [1:50:28<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_data_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Bio_sequence_Research_AITALAB\\\\train\\\\task1_splicing_prediction\\\\data_preparation\\\\train_val\\\\gtex_test_data_embeddings.pt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract test dataset embedding (run only one time)\n",
    "from extract_embed import process_csv_embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "NT_MODEL = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(NT_MODEL)\n",
    "model = AutoModel.from_pretrained(NT_MODEL).to(DEVICE)\n",
    "process_csv_embedding(csv_path=r'D:\\Bio_sequence_Research_AITALAB\\train\\task1_splicing_prediction\\data_preparation\\train_val\\gtex_test_data.csv', tokenizer=tokenizer, model=model, device=DEVICE, replace=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
