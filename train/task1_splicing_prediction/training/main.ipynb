{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22a9e6",
   "metadata": {
    "id": "4e22a9e6"
   },
   "source": [
    "# Task 1: Splicing Prediction Training\n",
    "## Enhanced with Experiment Tracking & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d798c",
   "metadata": {},
   "source": [
    "### 1) Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b72594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from config import (\n",
    "    DATA_DIR, SAVE_ROOT, BATCH_SIZE, LR, EPOCHS, PATIENCE,\n",
    "    HIDDEN_DIMS, DROPOUT, NUM_CLASSES, SEED, WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“¦ Configuration loaded:\")\n",
    "print(f\"  Data dir: {DATA_DIR}\")\n",
    "print(f\"  Save root: {SAVE_ROOT}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}, LR: {LR}, Epochs: {EPOCHS}\")\n",
    "print(f\"  Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3d6fd",
   "metadata": {
    "id": "8cf3d6fd"
   },
   "source": [
    "### 2) Train Splicing Models with Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639f0e",
   "metadata": {
    "id": "ae639f0e",
    "outputId": "7d7e84bd-df5f-4d6a-b23d-cbb8b89a1a5a"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from train import main as train_main\n",
    "\n",
    "# Create argument parser (simulating command-line args in notebook)\n",
    "parser = argparse.ArgumentParser(description=\"Task 1 Splicing Prediction Training\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE)\n",
    "parser.add_argument(\"--lr\", type=float, default=LR)\n",
    "parser.add_argument('--weight-decay', type=float, default=WEIGHT_DECAY)\n",
    "parser.add_argument(\"--epochs\", type=int, default=EPOCHS)\n",
    "parser.add_argument(\"--patience\", type=int, default=PATIENCE)\n",
    "parser.add_argument(\"--dropout\", type=float, default=DROPOUT)\n",
    "parser.add_argument(\"--device\", type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "parser.add_argument(\"--seed\", type=int, default=SEED)\n",
    "parser.add_argument(\"--data-dir\", type=str, default=DATA_DIR)\n",
    "parser.add_argument(\"--save-root\", type=str, default=SAVE_ROOT)\n",
    "parser.add_argument(\"--exp-num\", type=int, default=None)\n",
    "parser.add_argument(\"--ratio\", type=str, default=None, help=\"Specific ratio (e.g., 'ratio_10_80_10'). If not specified, train all ratios.\")\n",
    "parser.add_argument(\"--set\", type=str, default=None, help=\"Specific set (e.g., 'set_1'). If not specified, train all sets.\")\n",
    "\n",
    "# Parse empty args to use defaults, or modify as needed\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# Run training\n",
    "train_main(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97067d",
   "metadata": {},
   "source": [
    "### 3) View Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a70a17",
   "metadata": {
    "id": "75a70a17",
    "outputId": "1da2538f-0796-44e2-9060-0e0fd7e9715c"
   },
   "outputs": [],
   "source": [
    "# List all experiments and their results\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_experiment_results(exp_base_dir):\n",
    "    \"\"\"Load all experiment results from experiments directory.\"\"\"\n",
    "    if not os.path.exists(exp_base_dir):\n",
    "        print(f\"No experiments directory found at {exp_base_dir}\")\n",
    "        return None\n",
    "    \n",
    "    experiments = []\n",
    "    \n",
    "    for exp_name in sorted(os.listdir(exp_base_dir), reverse=True):\n",
    "        exp_dir = os.path.join(exp_base_dir, exp_name)\n",
    "        if not os.path.isdir(exp_dir):\n",
    "            continue\n",
    "        \n",
    "        # Load config\n",
    "        config_file = os.path.join(exp_dir, \"config.json\")\n",
    "        if not os.path.exists(config_file):\n",
    "            continue\n",
    "        \n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Aggregate results from all ratio/set combinations\n",
    "        avg_metrics = {}\n",
    "        num_sets = 0\n",
    "        \n",
    "        for ratio_folder in os.listdir(exp_dir):\n",
    "            ratio_path = os.path.join(exp_dir, ratio_folder)\n",
    "            if not os.path.isdir(ratio_path) or ratio_folder in ['tensorboard']:\n",
    "                continue\n",
    "            \n",
    "            for set_name in os.listdir(ratio_path):\n",
    "                set_path = os.path.join(ratio_path, set_name)\n",
    "                results_file = os.path.join(set_path, \"results.json\")\n",
    "                \n",
    "                if os.path.exists(results_file):\n",
    "                    with open(results_file, \"r\") as f:\n",
    "                        result_data = json.load(f)\n",
    "                        metrics = result_data.get('best_metrics', {})\n",
    "                        \n",
    "                        for key, val in metrics.items():\n",
    "                            if isinstance(val, (int, float)):\n",
    "                                avg_metrics[key] = avg_metrics.get(key, 0) + val\n",
    "                    \n",
    "                    num_sets += 1\n",
    "        \n",
    "        if num_sets > 0:\n",
    "            # Average the metrics\n",
    "            for key in avg_metrics:\n",
    "                avg_metrics[key] /= num_sets\n",
    "            \n",
    "            experiments.append({\n",
    "                'experiment': exp_name,\n",
    "                'num_sets': num_sets,\n",
    "                **{k: round(v, 4) for k, v in avg_metrics.items()}\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(experiments) if experiments else None\n",
    "\n",
    "# Display experiments\n",
    "exp_dir = SAVE_ROOT\n",
    "df_results = load_experiment_results(exp_dir)\n",
    "\n",
    "if df_results is not None:\n",
    "    print(\"ðŸ“Š Experiment Results Summary:\")\n",
    "    print(df_results.to_string())\n",
    "else:\n",
    "    print(\"No experiments found yet. Run training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d3f2b",
   "metadata": {},
   "source": [
    "### 4) TensorBoard Monitoring\n",
    "\n",
    "To monitor training progress in real-time:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir train/task1_splicing_prediction/trained_models/experiments/\n",
    "```\n",
    "\n",
    "Then open the URL in your browser to view:\n",
    "- Training/Validation loss curves\n",
    "- Metric evaluation (accuracy, precision, recall, F1, MCC, auc, balanced acc, specificity)\n",
    "- Model architecture summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8884451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_metric_comparison(exp_base_dir, metric='accuracy'):\n",
    "    \"\"\"Plot metric comparison across experiments.\"\"\"\n",
    "    if not os.path.exists(exp_base_dir):\n",
    "        print(\"No experiments found\")\n",
    "        return\n",
    "    \n",
    "    experiments = []\n",
    "    metrics_list = []\n",
    "    \n",
    "    for exp_name in sorted(os.listdir(exp_base_dir)):\n",
    "        exp_dir = os.path.join(exp_base_dir, exp_name)\n",
    "        if not os.path.isdir(exp_dir):\n",
    "            continue\n",
    "        \n",
    "        for ratio_folder in os.listdir(exp_dir):\n",
    "            ratio_path = os.path.join(exp_dir, ratio_folder)\n",
    "            if not os.path.isdir(ratio_path) or ratio_folder in ['tensorboard']:\n",
    "                continue\n",
    "            \n",
    "            for set_name in os.listdir(ratio_path):\n",
    "                set_path = os.path.join(ratio_path, set_name)\n",
    "                results_file = os.path.join(set_path, \"results.json\")\n",
    "                \n",
    "                if os.path.exists(results_file):\n",
    "                    with open(results_file, \"r\") as f:\n",
    "                        result_data = json.load(f)\n",
    "                        metrics_val = result_data.get('best_metrics', {}).get(metric, None)\n",
    "                        \n",
    "                        if metrics_val is not None:\n",
    "                            experiments.append(exp_name)\n",
    "                            metrics_list.append(metrics_val)\n",
    "    \n",
    "    if metrics_list:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(range(len(experiments)), metrics_list)\n",
    "        plt.xticks(range(len(experiments)), experiments, rotation=45, ha='right')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.title(f\"{metric.capitalize()} Comparison Across Experiments\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No {metric} data found\")\n",
    "\n",
    "# Example: plot accuracy comparison\n",
    "plot_metric_comparison(SAVE_ROOT, metric='accuracy')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
