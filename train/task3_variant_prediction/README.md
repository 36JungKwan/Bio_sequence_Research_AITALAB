# Task 3: Variant-Level Pathogenicity Prediction

Multi-modal fusion model Ä‘á»ƒ dá»± Ä‘oÃ¡n tÃ­nh gÃ¢y bá»‡nh (Pathogenicity) cá»§a biáº¿n thá»ƒ gen sá»­ dá»¥ng embedding tá»« Nucleotide Transformer (NT) vÃ  ESM-2.

## ğŸ“‹ Tá»•ng quan

Pipeline nÃ y thá»±c hiá»‡n:
1. **Data Preparation**: Split ClinVar variants theo chromosome (chr20/21 lÃ m test)
2. **Embedding Extraction**: TrÃ­ch embedding tá»« NT (DNA) vÃ  ESM-2 (Protein) - **zero-shot** (chÆ°a fine-tune)
3. **Multi-modal Fusion**: Káº¿t há»£p DNA + Protein embeddings Ä‘á»ƒ dá»± Ä‘oÃ¡n Pathogenic vs Benign
4. **Experiment Tracking**: Tá»± Ä‘á»™ng lÆ°u config, results, vÃ  model checkpoints cho má»—i experiment

## ğŸ—ï¸ Kiáº¿n trÃºc Model

```
Input Variants (ClinVar)
    â†“
[DNA Sequences] â†’ NT (zero-shot) â†’ E_dna_ref, E_dna_alt
[Protein Sequences] â†’ ESM-2 (zero-shot) â†’ E_prot_ref, E_prot_alt
    â†“
Fusion Layer: [E_ref, E_alt, E_alt - E_ref] per modality
    â†“
Concatenate DNA + Protein â†’ MLP Classifier
    â†“
Pathogenicity Score (Pathogenic=1, Benign=0)
```

## ğŸ“ Cáº¥u trÃºc ThÆ° má»¥c

```
task3_variant_prediction/
â”œâ”€â”€ config.py                    # Cáº¥u hÃ¬nh hyperparameters
â”œâ”€â”€ split_data.py                # Split parquet theo chromosome
â”œâ”€â”€ precompute_embeddings.py     # TrÃ­ch embedding NT + ESM-2
â”œâ”€â”€ dataset.py                  # PyTorch Dataset loader
â”œâ”€â”€ model.py                     # FusionClassifier model
â”œâ”€â”€ train.py                     # Training script vá»›i experiment tracking
â”œâ”€â”€ main.ipynb                   # Notebook Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline
â”œâ”€â”€ README.md                    # File nÃ y
â”‚
â”œâ”€â”€ data/                        # Split data (tá»± Ä‘á»™ng táº¡o)
â”‚   â”œâ”€â”€ train.parquet
â”‚   â”œâ”€â”€ val.parquet
â”‚   â””â”€â”€ test.parquet
â”‚
â”œâ”€â”€ embeddings/                   # Precomputed embeddings (tá»± Ä‘á»™ng táº¡o)
â”‚   â”œâ”€â”€ train_embeddings.pt
â”‚   â”œâ”€â”€ val_embeddings.pt
â”‚   â”œâ”€â”€ test_embeddings.pt
â”‚   â”œâ”€â”€ best_fusion_model.pt     # Global best model
â”‚   â””â”€â”€ experiments/             # Táº¥t cáº£ experiments
â”‚       â”œâ”€â”€ baseline_v1/
â”‚       â”‚   â”œâ”€â”€ config.json      # Config snapshot
â”‚       â”‚   â”œâ”€â”€ config.py         # Copy cá»§a config.py
â”‚       â”‚   â”œâ”€â”€ args.json         # Arguments Ä‘Ã£ dÃ¹ng
â”‚       â”‚   â”œâ”€â”€ results.json      # Test results
â”‚       â”‚   â”œâ”€â”€ best_model.pt     # Model checkpoint
â”‚       â”‚   â””â”€â”€ tensorboard/      # TensorBoard logs
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ runs/                        # TensorBoard logs
    â”œâ”€â”€ baseline_v1/
    â””â”€â”€ ...
```

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u

- Python 3.8+
- PyTorch (vá»›i CUDA náº¿u cÃ³ GPU)
- Transformers (HuggingFace)
- CÃ¡c packages khÃ¡c: `pandas`, `numpy`, `torchmetrics`, `tensorboard`, `tqdm`, `seaborn`, `matplotlib`

### CÃ i Ä‘áº·t dependencies

```bash
pip install torch torchvision torchaudio
pip install transformers
pip install pandas numpy torchmetrics tensorboard tqdm seaborn matplotlib
```

## ğŸ“– HÆ°á»›ng dáº«n Sá»­ dá»¥ng

### CÃ¡ch 1: Sá»­ dá»¥ng Notebook (Khuyáº¿n nghá»‹)

1. **Má»Ÿ `main.ipynb`** vÃ  cháº¡y tuáº§n tá»± cÃ¡c cells:

   **Cell 1-2: Split Data**
   ```python
   from split_data import main as split_main
   from config import RAW_PARQUET
   split_main(RAW_PARQUET)
   ```
   - Lá»c chá»‰ giá»¯ `Pathogenic` vÃ  `Benign` variants
   - Split: chr20/21 â†’ test, cÃ²n láº¡i â†’ train/val (15% val)

   **Cell 3-4: Precompute Embeddings**
   ```python
   %env TOKENIZERS_PARALLELISM=false
   from precompute_embeddings import main as emb_main
   emb_main()
   ```
   - TrÃ­ch embedding tá»« NT (DNA) vÃ  ESM-2 (Protein)
   - LÆ°u vÃ o `.pt` files (cÃ³ thá»ƒ máº¥t vÃ i phÃºt/giá» tÃ¹y GPU)

   **Cell 5-6: Train Model**
   ```python
   from train import train
   from config import LR, EPOCHS, BATCH_SIZE, PATIENCE, DROPOUT, SEED
   import argparse

   parser = argparse.ArgumentParser()
   parser.add_argument("--lr", type=float, default=LR)
   parser.add_argument("--epochs", type=int, default=EPOCHS)
   parser.add_argument("--batch_size", type=int, default=BATCH_SIZE)
   parser.add_argument("--patience", type=int, default=PATIENCE)
   parser.add_argument("--dropout", type=float, default=DROPOUT)
   parser.add_argument("--seed", type=int, default=SEED)
   parser.add_argument("--exp_name", type=str, default="baseline_v1")
   parser.add_argument("--log_dir", type=str, default=None)

   args = parser.parse_args([])
   result = train(args)
   ```

   **Cell 7-8: Xem láº¡i Experiments**
   - Xem danh sÃ¡ch táº¥t cáº£ experiments
   - Xem chi tiáº¿t má»™t experiment cá»¥ thá»ƒ

### CÃ¡ch 2: Sá»­ dá»¥ng Command Line

```bash
# 1. Split data
python split_data.py --parquet <path_to_parquet>

# 2. Precompute embeddings
python precompute_embeddings.py

# 3. Train vá»›i exp_name cá»¥ thá»ƒ
python train.py --exp_name baseline_v1 --lr 1e-3 --dropout 0.2

# 4. Train vá»›i exp_name khÃ¡c, override config
python train.py --exp_name experiment_v2 --lr 5e-4 --dropout 0.3 --patience 3
```

## ğŸ¯ CÃ¡c TÃ­nh nÄƒng ChÃ­nh

### 1. **Auto Experiment Tracking**

Má»—i láº§n train, há»‡ thá»‘ng tá»± Ä‘á»™ng:
- Táº¡o thÆ° má»¥c `embeddings/experiments/<exp_name>/`
- LÆ°u config snapshot (`config.json`)
- LÆ°u arguments Ä‘Ã£ dÃ¹ng (`args.json`)
- LÆ°u test results (`results.json`)
- Copy model checkpoint (`best_model.pt`)
- Copy TensorBoard logs

### 2. **Experiment Naming**

- **CÃ³ thá»ƒ Ä‘áº·t tÃªn**: `--exp_name baseline_v1`
- **Tá»± Ä‘á»™ng táº¡o**: Náº¿u khÃ´ng cÃ³, táº¡o theo timestamp `exp_20241201_143022`

### 3. **Configuration Management**

- Táº¥t cáº£ hyperparameters trong `config.py`
- CÃ³ thá»ƒ override tá»« command line hoáº·c notebook
- Má»—i experiment lÆ°u snapshot config Ä‘á»ƒ reproduce

### 4. **TensorBoard Integration**

- Tá»± Ä‘á»™ng log: loss, metrics, confusion matrices
- HPARAMS tab: So sÃ¡nh hyperparameters vÃ  metrics giá»¯a experiments
- Xem: `tensorboard --logdir runs`

### 5. **Multi-modal Fusion**

- **DNA Branch**: NT embedding cho `ref_seq` vÃ  `alt_seq` (601bp, center token)
- **Protein Branch**: ESM-2 embedding cho `prot_ref_seq` vÃ  `prot_alt_seq` (101aa, center token)
- **Fusion**: `[E_ref, E_alt, E_alt - E_ref]` per modality â†’ Concatenate â†’ MLP

## ğŸ“Š Xem láº¡i Káº¿t quáº£

### 1. Trong Notebook

**Xem danh sÃ¡ch táº¥t cáº£ experiments:**
```python
# Cell 8 trong main.ipynb
# Hiá»ƒn thá»‹ báº£ng: exp_name | timestamp | test_auc | test_acc | test_mcc | ...
```

**Xem chi tiáº¿t má»™t experiment:**
```python
# Cell 9 trong main.ipynb
exp_name = "baseline_v1"  # Thay báº±ng exp_name báº¡n muá»‘n
# Hiá»ƒn thá»‹ config vÃ  results chi tiáº¿t
```

### 2. TensorBoard

```bash
# Xem táº¥t cáº£ experiments
tensorboard --logdir train/task3_variant_prediction/runs

# Trong TensorBoard:
# - SCALARS: Loss/metrics curves
# - HPARAMS: So sÃ¡nh hyperparameters vÃ  metrics
# - IMAGES: Confusion matrices
```

### 3. Äá»c File JSON

```python
import json

# Äá»c config
with open("embeddings/experiments/baseline_v1/config.json", "r") as f:
    config = json.load(f)

# Äá»c results
with open("embeddings/experiments/baseline_v1/results.json", "r") as f:
    results = json.load(f)
```

## ğŸ”§ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a `config.py` Ä‘á»ƒ thay Ä‘á»•i:

```python
# Models
NT_MODEL = "InstaDeepAI/nucleotide-transformer-500m-human-ref"
ESM_MODEL = "facebook/esm2_t33_650M_UR50D"

# Sequence lengths
DNA_SEQ_LEN = 601
PROT_SEQ_LEN = 101

# Training hyperparameters
PROJ_DIM = 512
FUSION_HIDDEN = [512, 256]
DROPOUT = 0.2
LR = 1e-3
EPOCHS = 30
PATIENCE = 5
BATCH_SIZE = 128

# Data split
TEST_CHROMS = {"chr20", "chr21", "20", "21"}
VAL_RATIO = 0.15
```

## ğŸ“ VÃ­ dá»¥ Workflow

### Experiment 1: Baseline
```python
parser.add_argument("--exp_name", type=str, default="baseline_v1")
# Káº¿t quáº£: AUC=0.9850, Acc=0.9397
```

### Experiment 2: TÄƒng Dropout (giáº£m overfitting)
```python
parser.add_argument("--exp_name", type=str, default="baseline_v2_dropout03")
parser.add_argument("--dropout", type=float, default=0.3)
# So sÃ¡nh vá»›i baseline_v1
```

### Experiment 3: Giáº£m Learning Rate
```python
parser.add_argument("--exp_name", type=str, default="baseline_v3_lr5e4")
parser.add_argument("--lr", type=float, default=5e-4)
# So sÃ¡nh vá»›i cÃ¡c experiments trÆ°á»›c
```

### So sÃ¡nh trong TensorBoard:
```bash
tensorboard --logdir runs
# Tab HPARAMS â†’ Chá»n experiments â†’ Parallel coordinates plot
```

## ğŸ“ˆ Metrics

Model Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng:
- **AUC** (Area Under ROC Curve)
- **Accuracy**
- **MCC** (Matthews Correlation Coefficient)
- **F1 Score** (macro/micro)
- **Confusion Matrix**

## âš ï¸ LÆ°u Ã½

1. **GPU Memory**: ESM-2 t33_650M cáº§n ~16GB+ VRAM. Náº¿u thiáº¿u, giáº£m `PROT_BATCH` trong `config.py`
2. **Precompute Time**: TrÃ­ch embedding cÃ³ thá»ƒ máº¥t vÃ i giá» tÃ¹y sá»‘ lÆ°á»£ng variants vÃ  GPU
3. **Overfitting**: Náº¿u tháº¥y train loss giáº£m nhÆ°ng val loss tÄƒng â†’ tÄƒng dropout, giáº£m LR, hoáº·c thÃªm regularization
4. **Data Path**: Cáº­p nháº­t `RAW_PARQUET` trong `config.py` trÆ°á»›c khi cháº¡y

## ğŸ› Troubleshooting

### Lá»—i: Out of Memory
- Giáº£m `BATCH_SIZE` trong `config.py`
- Giáº£m `PROT_BATCH` (cho ESM-2)
- Giáº£m `DNA_BATCH` (cho NT)

### Lá»—i: File not found
- Kiá»ƒm tra `RAW_PARQUET` path trong `config.py`
- Äáº£m báº£o Ä‘Ã£ cháº¡y `split_data.py` trÆ°á»›c `precompute_embeddings.py`

### Lá»—i: CUDA out of memory
- Giáº£m batch sizes
- DÃ¹ng CPU: `device = "cpu"` (sáº½ cháº­m hÆ¡n nhiá»u)

## ğŸ“š TÃ i liá»‡u Tham kháº£o

- **Nucleotide Transformer**: [InstaDeepAI/nucleotide-transformer](https://huggingface.co/InstaDeepAI/nucleotide-transformer-500m-human-ref)
- **ESM-2**: [facebook/esm2](https://huggingface.co/facebook/esm2_t33_650M_UR50D)
- **ClinVar**: [NCBI ClinVar](https://www.ncbi.nlm.nih.gov/clinvar/)

## ğŸ“„ License

Xem LICENSE file trong repository chÃ­nh.

---

**TÃ¡c giáº£**: Bio Sequence Research Team  
