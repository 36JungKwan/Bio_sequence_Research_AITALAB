{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac57de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2, 2048, 1024)\n",
      "Embeddings per token: [[[ 0.5063792   0.14925258  0.5267063  ... -0.601356    0.7671192\n",
      "   -0.24135408]\n",
      "  [ 0.3073924   0.01497287  0.31069636 ... -0.86925024  0.70535207\n",
      "   -0.04934708]\n",
      "  [ 0.27530265  0.23837054  0.16277047 ... -0.95569     0.6267978\n",
      "   -0.07544966]\n",
      "  ...\n",
      "  [ 0.19590834 -0.07999256  0.1046586  ...  0.00263992  0.46054572\n",
      "   -0.035153  ]\n",
      "  [ 0.337595   -0.2000117   0.04658737 ...  0.02611889  0.44137904\n",
      "   -0.21832097]\n",
      "  [ 0.07733975 -0.34249914  0.07812422 ...  0.20739084  0.28529444\n",
      "   -0.15824527]]\n",
      "\n",
      " [[-0.0262093  -0.5077783   0.35803428 ... -0.23095986  0.786347\n",
      "   -1.0120146 ]\n",
      "  [ 0.49354613 -0.59404874  0.2465686  ... -0.33584282  0.06687836\n",
      "    0.25064915]\n",
      "  [ 0.01741505  0.09177446  0.43233347 ... -0.2013575   0.3341403\n",
      "    0.0131175 ]\n",
      "  ...\n",
      "  [ 0.20584804 -0.7662893   0.5937591  ... -0.5139867   0.6144493\n",
      "   -1.1537112 ]\n",
      "  [-0.07695537 -0.36022472 -0.3743491  ... -0.6763517   0.42619184\n",
      "   -1.0293761 ]\n",
      "  [-0.68401927 -0.58734846  0.29781926 ... -0.6808119   0.65890306\n",
      "   -1.0347176 ]]]\n",
      "Mean sequence embeddings: [[ 0.36339918  0.14371049  0.28098249 ... -0.86226189  0.64218724\n",
      "  -0.10281314]\n",
      " [ 0.30413461 -0.15126516  0.362372   ... -0.07436064  0.08204038\n",
      "  -0.29404691]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "# Import the tokenizer and the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\", trust_remote_code=True).to(device)\n",
    "\n",
    "# Choose the length to which the input sequences are padded. By default, the \n",
    "# model max length is chosen, but feel free to decrease it as the time taken to \n",
    "# obtain the embeddings increases significantly with it.\n",
    "max_length = tokenizer.model_max_length\n",
    "\n",
    "# Create a dummy dna sequence and tokenize it\n",
    "sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "tokens_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\", padding=\"max_length\", max_length = max_length)[\"input_ids\"].to(device)\n",
    "\n",
    "# Compute the embeddings\n",
    "attention_mask = (tokens_ids != tokenizer.pad_token_id).to(device)\n",
    "torch_outs = model(\n",
    "    tokens_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_attention_mask=attention_mask,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Compute sequences embeddings\n",
    "embeddings = torch_outs['hidden_states'][-1].detach().cpu().numpy()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings per token: {embeddings}\")\n",
    "\n",
    "# Add embed dimension axis\n",
    "attention_mask = torch.unsqueeze(attention_mask, dim=-1).cpu().numpy()\n",
    "\n",
    "# Compute mean embeddings per sequence\n",
    "mean_sequence_embeddings = np.sum(attention_mask*embeddings, axis=-2)/np.sum(attention_mask, axis=1)\n",
    "print(f\"Mean sequence embeddings: {mean_sequence_embeddings}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37add104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (2, 1000, 2560)\n",
      "Embeddings per token: [[[-0.00180363  0.07674704  0.01244043 ...  0.00112825  0.03381748\n",
      "    0.17844304]\n",
      "  [-0.09584967  0.2575793  -0.0365719  ... -0.44206974  0.06707363\n",
      "    0.22186989]\n",
      "  [-0.25077254  0.07775501 -0.05562799 ... -0.06127694  0.16998267\n",
      "    0.2654602 ]\n",
      "  ...\n",
      "  [-0.06766268  0.0765079   0.00185914 ...  0.02911628  0.15405582\n",
      "    0.14926068]\n",
      "  [-0.06766268  0.0765079   0.00185914 ...  0.02911628  0.15405582\n",
      "    0.14926068]\n",
      "  [-0.06766268  0.0765079   0.00185914 ...  0.02911628  0.15405582\n",
      "    0.14926068]]\n",
      "\n",
      " [[ 0.01480382  0.09288551  0.02611541 ... -0.0242852   0.02126851\n",
      "    0.20392066]\n",
      "  [-0.24921286  0.5893351   0.440742   ... -0.39238933  0.5610121\n",
      "    0.41510585]\n",
      "  [-0.36850718  0.32135677  0.3190047  ... -0.4604549   0.36477548\n",
      "    0.5401475 ]\n",
      "  ...\n",
      "  [ 0.01522855  0.08277307 -0.08055211 ... -0.10329894  0.16529028\n",
      "    0.21183467]\n",
      "  [ 0.01522855  0.08277307 -0.08055211 ... -0.10329894  0.16529028\n",
      "    0.21183467]\n",
      "  [ 0.01522855  0.08277307 -0.08055211 ... -0.10329894  0.16529028\n",
      "    0.21183467]]]\n",
      "Mean sequence embeddings: [[-0.04364849  0.11171833 -0.01340149 ... -0.22690335  0.07551927\n",
      "   0.14327295]\n",
      " [-0.11648337  0.22688601  0.12387154 ... -0.31522706  0.30403643\n",
      "   0.52010573]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Import the tokenizer and the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\").to(device)\n",
    "\n",
    "# Choose the length to which the input sequences are padded. By default, the \n",
    "# model max length is chosen, but feel free to decrease it as the time taken to \n",
    "# obtain the embeddings increases significantly with it.\n",
    "max_length = tokenizer.model_max_length\n",
    "\n",
    "# Create a dummy dna sequence and tokenize it\n",
    "sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "tokens_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\", padding=\"max_length\", max_length = max_length)[\"input_ids\"].to(device)\n",
    "\n",
    "# Compute the embeddings\n",
    "attention_mask = (tokens_ids != tokenizer.pad_token_id).to(device)\n",
    "torch_outs = model(\n",
    "    tokens_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    encoder_attention_mask=attention_mask,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Compute sequences embeddings\n",
    "embeddings = torch_outs['hidden_states'][-1].detach().cpu().numpy()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings per token: {embeddings}\")\n",
    "\n",
    "# Add embed dimension axis\n",
    "attention_mask = torch.unsqueeze(attention_mask, dim=-1).cpu().numpy()\n",
    "\n",
    "# Compute mean embeddings per sequence\n",
    "mean_sequence_embeddings = np.sum(attention_mask*embeddings, axis=-2)/np.sum(attention_mask, axis=1)\n",
    "print(f\"Mean sequence embeddings: {mean_sequence_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a416ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant effect score: 2.7199\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# ==== Load model and tokenizer ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# ==== Input DNA sequences ====\n",
    "ref_seq = \"ATGCGTACGATCGTACGATCGTACG\"\n",
    "mut_seq = \"ATGCGTACGATTGTACGATCGTACG\"   # M·ªôt bi·∫øn d·ªã nh·ªè so v·ªõi ref_seq\n",
    "\n",
    "# ==== Tokenize & encode ====\n",
    "inputs = tokenizer([ref_seq, mut_seq], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# ==== Forward pass ====\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    embeds = outputs.hidden_states[-1].mean(dim=1)  # Mean pooling to√†n chu·ªói\n",
    "\n",
    "# ==== Compute variant effect ====\n",
    "# Vector hi·ªáu gi·ªØa embedding c·ªßa chu·ªói ƒë·ªôt bi·∫øn v√† tham chi·∫øu\n",
    "delta = embeds[1] - embeds[0]\n",
    "score = torch.norm(delta, p=2).item()\n",
    "\n",
    "print(f\"Variant effect score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db5b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 18 downstream tasks:\n",
      "['H3K4me3', 'enhancers_types', 'H3K4me1', 'splice_sites_acceptors', 'promoter_tata', 'H2AFZ', 'H3K27ac', 'H3K9ac', 'promoter_no_tata', 'H3K36me3', 'enhancers', 'splice_sites_all', 'promoter_all', 'H3K27me3', 'H3K4me2', 'splice_sites_donors', 'H3K9me3', 'H4K20me1']\n",
      "\n",
      "üöÄ Benchmarking task: H3K4me3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 319384.53 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2184/2184 [02:20<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K4me3: MCC=0.644, F1=0.821, ACC=0.821\n",
      "\n",
      "üöÄ Benchmarking task: enhancers_types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 341643.00 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [01:52<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ enhancers_types: MCC=0.416, F1=0.500, ACC=0.695\n",
      "\n",
      "üöÄ Benchmarking task: H3K4me1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 357895.66 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [04:00<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K4me1: MCC=0.407, F1=0.703, ACC=0.703\n",
      "\n",
      "üöÄ Benchmarking task: splice_sites_acceptors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 328779.45 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [02:19<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ splice_sites_acceptors: MCC=0.464, F1=0.732, ACC=0.732\n",
      "\n",
      "üöÄ Benchmarking task: promoter_tata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 338114.98 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [00:16<00:00, 37.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ promoter_tata: MCC=0.633, F1=0.816, ACC=0.816\n",
      "\n",
      "üöÄ Benchmarking task: H2AFZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 350564.43 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:59<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H2AFZ: MCC=0.406, F1=0.703, ACC=0.703\n",
      "\n",
      "üöÄ Benchmarking task: H3K27ac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 335879.52 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:59<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K27ac: MCC=0.432, F1=0.716, ACC=0.716\n",
      "\n",
      "üöÄ Benchmarking task: H3K9ac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 334937.84 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2910/2910 [03:05<00:00, 15.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K9ac: MCC=0.492, F1=0.746, ACC=0.746\n",
      "\n",
      "üöÄ Benchmarking task: promoter_no_tata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 333924.72 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [01:39<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ promoter_no_tata: MCC=0.738, F1=0.867, ACC=0.868\n",
      "\n",
      "üöÄ Benchmarking task: H3K36me3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 336985.06 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:57<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K36me3: MCC=0.513, F1=0.756, ACC=0.756\n",
      "\n",
      "üöÄ Benchmarking task: enhancers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 347638.11 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [01:49<00:00, 34.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ enhancers: MCC=0.451, F1=0.725, ACC=0.725\n",
      "\n",
      "üöÄ Benchmarking task: splice_sites_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 347855.20 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [02:17<00:00, 27.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ splice_sites_all: MCC=0.366, F1=0.576, ACC=0.577\n",
      "\n",
      "üöÄ Benchmarking task: promoter_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 342281.44 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [01:39<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ promoter_all: MCC=0.717, F1=0.857, ACC=0.857\n",
      "\n",
      "üöÄ Benchmarking task: H3K27me3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 344383.53 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:58<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K27me3: MCC=0.529, F1=0.764, ACC=0.764\n",
      "\n",
      "üöÄ Benchmarking task: H3K4me2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 349341.84 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:58<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K4me2: MCC=0.482, F1=0.741, ACC=0.741\n",
      "\n",
      "üöÄ Benchmarking task: splice_sites_donors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 345751.79 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [02:17<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ splice_sites_donors: MCC=0.524, F1=0.762, ACC=0.762\n",
      "\n",
      "üöÄ Benchmarking task: H3K9me3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 346728.56 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3430/3430 [03:37<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H3K9me3: MCC=0.266, F1=0.632, ACC=0.633\n",
      "\n",
      "üöÄ Benchmarking task: H4K20me1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493242/493242 [00:01<00:00, 344962.99 examples/s]\n",
      "Embedding batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3750/3750 [03:58<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ H4K20me1: MCC=0.585, F1=0.792, ACC=0.792\n",
      "\n",
      "üéØ Benchmark ho√†n t·∫•t! K·∫øt qu·∫£ l∆∞u t·∫°i: nt_benchmark_results.csv\n",
      "                             MCC        F1       ACC\n",
      "H3K4me3                 0.643520  0.820750  0.821102\n",
      "enhancers_types         0.415666  0.500398  0.695400\n",
      "H3K4me1                 0.406888  0.702627  0.702967\n",
      "splice_sites_acceptors  0.464088  0.731991  0.732067\n",
      "promoter_tata           0.632878  0.815886  0.816278\n",
      "H2AFZ                   0.406415  0.703112  0.703167\n",
      "H3K27ac                 0.432353  0.716110  0.716167\n",
      "H3K9ac                  0.492102  0.745833  0.745940\n",
      "promoter_no_tata        0.738042  0.867467  0.867733\n",
      "H3K36me3                0.512624  0.756067  0.756133\n",
      "enhancers               0.450917  0.724940  0.725167\n",
      "splice_sites_all        0.366398  0.576175  0.577267\n",
      "promoter_all            0.716736  0.856815  0.857067\n",
      "H3K27me3                0.529424  0.764217  0.764333\n",
      "H3K4me2                 0.481797  0.740797  0.740833\n",
      "splice_sites_donors     0.524423  0.762113  0.762167\n",
      "H3K9me3                 0.266016  0.632431  0.632735\n",
      "H4K20me1                0.584586  0.792210  0.792267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# 1Ô∏è‚É£ Setup model & tokenizer\n",
    "# ==============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Load benchmark dataset\n",
    "# ==============================\n",
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\")\n",
    "task_names = list(set(dataset[\"train\"][\"task\"]))  # L·∫•y danh s√°ch 18 task\n",
    "\n",
    "print(f\"‚úÖ Found {len(task_names)} downstream tasks:\")\n",
    "print(task_names)\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£ H√†m tr√≠ch xu·∫•t embedding\n",
    "# ==============================\n",
    "@torch.no_grad()\n",
    "def get_embeddings(seqs, batch_size=4):\n",
    "    \"\"\"Tr√≠ch embedding mean-pooling cho list chu·ªói DNA\"\"\"\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(seqs), batch_size), desc=\"Embedding batch\"):\n",
    "        batch = seqs[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outs = model(**tokens, output_hidden_states=True)\n",
    "        emb = outs.hidden_states[-1].mean(dim=1).cpu()  # mean pooling to√†n chu·ªói\n",
    "        all_embs.append(emb)\n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Pipeline benchmark\n",
    "# ==============================\n",
    "results = {}\n",
    "\n",
    "for task in task_names:\n",
    "    print(f\"\\nüöÄ Benchmarking task: {task}\")\n",
    "\n",
    "    # --- L·ªçc d·ªØ li·ªáu theo task ---\n",
    "    data = dataset[\"train\"].filter(lambda x: x[\"task\"] == task)\n",
    "    seqs = data[\"sequence\"]\n",
    "    labels = np.array(data[\"label\"])\n",
    "\n",
    "    # --- Tr√≠ch xu·∫•t embedding ---\n",
    "    embs = get_embeddings(seqs, batch_size=8).numpy()\n",
    "\n",
    "    # --- Cross-validation 10-fold ---\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    scores = {\"MCC\": [], \"F1\": [], \"ACC\": []}\n",
    "\n",
    "    for train_idx, test_idx in kf.split(embs):\n",
    "        X_train, X_test = embs[train_idx], embs[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "        clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "\n",
    "        scores[\"MCC\"].append(matthews_corrcoef(y_test, preds))\n",
    "        scores[\"F1\"].append(f1_score(y_test, preds, average=\"macro\"))\n",
    "        scores[\"ACC\"].append(accuracy_score(y_test, preds))\n",
    "\n",
    "    # --- Trung b√¨nh ƒëi·ªÉm ---\n",
    "    results[task] = {m: np.mean(v) for m, v in scores.items()}\n",
    "    print(f\"‚úÖ {task}: MCC={results[task]['MCC']:.3f}, F1={results[task]['F1']:.3f}, ACC={results[task]['ACC']:.3f}\")\n",
    "\n",
    "# ==============================\n",
    "# 5Ô∏è‚É£ Xu·∫•t k·∫øt qu·∫£ t·ªïng h·ª£p\n",
    "# ==============================\n",
    "df = pd.DataFrame(results).T\n",
    "#df.to_csv(\"nt_benchmark_results.csv\")\n",
    "print(\"\\nüéØ Benchmark ho√†n t·∫•t! K·∫øt qu·∫£ l∆∞u t·∫°i: nt_benchmark_results.csv\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dung\\anaconda3\\envs\\stable\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at InstaDeepAI/nucleotide-transformer-v2-500m-multi-species were not used when initializing EsmForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-v2-500m-multi-species and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  101/30000 11:34:22 < 3495:09:28, 0.00 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Setup model & tokenizer\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # ƒëi·ªÅu ch·ªânh theo task (binary ho·∫∑c multi-class)\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Load 1 downstream task\n",
    "# =========================\n",
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\")\n",
    "task_name = \"promoter_all\"  # v√≠ d·ª•\n",
    "task_ds = dataset[\"train\"].filter(lambda x: x[\"task\"] == task_name)\n",
    "\n",
    "# T√°ch train/test\n",
    "train_size = int(0.8 * len(task_ds))\n",
    "train_ds = task_ds.select(range(train_size))\n",
    "test_ds = task_ds.select(range(train_size, len(task_ds)))\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Tokenization\n",
    "# =========================\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"sequence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Define training arguments\n",
    "# =========================\n",
    "'''training_args = TrainingArguments(\n",
    "    output_dir=\"./nt_finetune_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # b·∫≠t mixed precision n·∫øu c√≥ GPU h·ªó tr·ª£\n",
    "    save_total_limit=2\n",
    ")'''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nt_finetune_results\",\n",
    "    do_eval=True,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Define metrics\n",
    "# =========================\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"mcc\": matthews_corrcoef(labels, preds),\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Fine-tune model\n",
    "# =========================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ Evaluate\n",
    "# =========================\n",
    "results = trainer.evaluate()\n",
    "print(f\"‚úÖ Fine-tune done for task {task_name}\")\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
